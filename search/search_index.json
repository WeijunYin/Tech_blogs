{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Wei-Jun Yin's blog # Hi, I'm Weijun Yin, a phd student in Wuhan University. I'm interested in time series analysis in wind energy. My personal website is here . For full documentation visit mkdocs.org .","title":"Home"},{"location":"#wei-jun-yins-blog","text":"Hi, I'm Weijun Yin, a phd student in Wuhan University. I'm interested in time series analysis in wind energy. My personal website is here . For full documentation visit mkdocs.org .","title":"Wei-Jun Yin's blog"},{"location":"Research/Ch1/","text":"If we can","title":"How to Write"},{"location":"Research/Ch2/","text":"\\[ \\mathrm{v}_{\\mathrm{i}}^{\\mathrm{H}}(\\mathrm{dA}) \\mathrm{v}_{\\mathrm{i}}+\\mathrm{v}_{\\mathrm{i}}^{\\mathrm{H}} A \\mathrm{Ad} \\mathrm{v}_{\\mathrm{i}}=\\mathrm{v}_{\\mathrm{i}}^{\\mathrm{H}}\\left(\\mathrm{d} \\lambda_{\\mathrm{i}}\\right) \\mathrm{v}_{\\mathrm{i}}+\\mathrm{v}_{\\mathrm{i}}^{\\mathrm{H}} \\lambda_{\\mathrm{i}}\\left(\\mathrm{dv}_{\\mathrm{i}}\\right) \\]","title":"Second Part"},{"location":"Research/DataLoader/","text":"DataLoader\u7684\u6784\u5efa # \u5bfc\u5165\u5fc5\u8981\u7684\u5305 # # \u8bfb\u53d6\u6587\u4ef6 import os # \u8fdb\u884c\u5fc5\u8981\u7684\u8ba1\u7b97 import numpy as np # \u753b\u56fe import pandas as pd # \u5bfc\u5165torch import torch # \u5bfc\u5165Dataset, \u8fd9\u91cc\u5176\u5b9e\u6211\u4eec\u9700\u8981\u81ea\u5b9a\u4e49 # \u5bfc\u5165DataLoader, \u7528\u6765\u6279\u6b21\u8bfb\u53d6\u6570\u636e from torch.utils.data import Dataset, DataLoader # \u5f52\u4e00\u5316\u5904\u7406\uff0c\u907f\u514d\u635f\u5931\u7684\u503c\u8fc7\u5927 from sklearn.preprocessing import StandardScaler # \u5bfc\u5165utils\u5f53\u4e2d\u4e3a\u628a\u65f6\u95f4\u6233\u5316\u4e3a\u7279\u5f81\u7684\u51fd\u6570 from utils.timefeatures import time_features # \u5ffd\u89c6\u635f\u5931 import warnings warnings.filterwarnings('ignore') \u4e3a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u6784\u9020\u4e0d\u540c\u7684\u6570\u636e\u96c6MyDataset, \u8fd9\u662f\u7531\u4e8e\u9700\u8981\u8bfb\u53d6\u4e0d\u540c\u7c7b\u578b\u7684\u6570\u636e\u96c6 # \u6bcf\u4e2a\u6570\u636e\u96c6\u9700\u8981\u6784\u5efa\u56db\u4e2a\u51fd\u6570 __init__(self, xxx,yyy): \u521d\u59cb\u5316\uff0c\u5bfc\u5165\u53c2\u6570 __read_data__(self): \u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4ee5\u53ca\u7279\u5f81\u5de5\u7a0b\u4ee5\u53ca\u5212\u5206\u6570\u636e\u96c6(\u9884\u5904\u7406\u7684\u90e8\u5206) __getitem__(self,index):\u7528\u4e8e\u9010\u4e2a\u8bfb\u53d6\u6837\u672c\u96c6\u5408\u4e2d\u7684\u5143\u7d20\uff0c\u53ef\u4ee5\u8fdb\u884c\u4e00\u5b9a\u7684\u53d8\u6362\uff0c\u5e76\u5c06\u8fd4\u56de\u6570\u636e\u96c6\u548c\u9a8c\u8bc1\u96c6\u6240\u9700\u8981\u7684\u6570\u636e __len__(self): \u8fd4\u56de\u6570\u636e\u96c6\u7684\u6837\u672c\u6570 __inverse_transform(self,data): \u8fd4\u56de\u53cd-\u5f52\u4e00\u5316\u7684\u6570\u636e class MyDataset(Dataset): \u7b2c\u4e00\u6b65\uff1a\u521d\u59cb\u5316\u6709\u54ea\u4e9b\u53c2\u6570 # def __init__(self, directory_path, Train_or_Test='train', size=None, features='S', data_name='Dataset_name.csv', predicted_col='OT', scale=True, inverse=False, timeenc=0, freq='h',cols=None) #def __init__(self, root_path, flag='train', size=None, # features='S', data_path='ETTh1.csv', # target='OT', scale=True, timeenc=0, freq='h', train_only=False): # size: # size[0]: \u4f20\u5230\u7f16\u7801\u5668\u5e8f\u5217\u7684\u957f\u5ea6 # size[1]: \u4f20\u5165\u5230\u89e3\u7801\u5668\u7684start token\u7684\u957f\u5ea6 # size[2]: \u9700\u8981\u9884\u6d4b\u7684\u5e8f\u5217\u957f\u5ea6 # \u628a\u7c7b\u578b\u75280\uff0c 1, 2\u8868\u793a assert flag in ['train', 'test', 'val'] type_map={'train':0, 'val':1, 'test':2} self.set_type=type_map[flag] # \u5bfc\u5165\u53c2\u6570 self.features =features self.predicted_col = predicted_col self.scale=scale self.inverse=inverse self.timeenc=timeenc self.freq=freq #\u6570\u636e\u7c92\u5ea6 self.root_path=root_path # \u5bfc\u5165\u6570\u636e\u96c6\u6240\u5728\u7684\u6587\u4ef6\u5939 self.data_path=data_path # \u5bfc\u5165\u6570\u636e\u96c6\u7684\u540d\u79f0 self.__read_data__() \u7b2c\u4e8c\u6b65\uff0c\u521d\u59cb\u5316\u65f6\u5019\u6700\u91cd\u8981\u7684\u51fd\u6570 # __read_data_ def __read_data__(self): # \u5148\u51c6\u5907\u6570\u636e\u5f52\u4e00\u5316 self.scaler = StandardScaler() # \u8bfb\u53d6\u6570\u636e df_raw = pd.read_csv(os.path.join(self.root_path, self.data_path)) # \u73b0\u6839\u636e train, val, test\u6765\u628a\u5bf9\u5e94\u6570\u636e\u96c6\u5728\u539f\u59cb\u6570\u636e\u96c6\u7684index\u5212\u5206\u51fa\u6765 # {'train':0, 'val':1, 'test':2} border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len] border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24] # \u6839\u636e\u9884\u6d4b\u7684\u7c7b\u578b\u8fdb\u4e00\u6b65\u63d0\u53d6\u6570\u636e # M:\u591a\u5143\u9884\u6d4b\u591a\u5143 # S: \u5355\u5143\u9884\u6d4b\u5355\u5143 # MS: \u591a\u5143\u9884\u6d4b\u5355\u5143 if self.features == 'M' or self.features == 'MS': cols_data = df_raw.columns[1:] #\u56e0\u4e3a df_raw.columns[0]\u662fdate df_data = df_raw[cols_data] elif self.features == 'S': df_data = df_raw[[self.target]] # \u53ea\u7528\u5f85\u9884\u6d4b\u7684\u91cf\u6765\u9884\u6d4b\uff0c\u6240\u4ee5\u662ftarget # \u5982\u679c\u9700\u8981\u8fdb\u884c\u5f52\u4e00\u5316 # scale=None if self.scale: train_data = df_data[border1s[0]:border2s[0]] self.scaler.fit(train_data.values) data = self.scaler.transform(df_data.values) else: data = df_data.values # \u628a\u6570\u636e\u8f6c\u5316\u4e3anumpy\u6570\u7ec4 # \u53d6\u51fa\u5236\u5b9a\u8303\u56f4\u5185\u7684\u5e8f\u5217\u6570\u636e\u7684date\u5217 df_stamp = df_raw[['date']][border1:border2] df_stamp['date'] = pd.to_datetime(df_stamp.date) # \u5229\u7528 time_feature\u628adate\u6570\u636e\u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u5904\u7406 if self.timeenc == 0: df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1) df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1) df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1) df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1) data_stamp = df_stamp.drop(['date'], 1).values elif self.timeenc == 1: data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq) data_stamp = data_stamp.transpose(1, 0) # \u6709\u65f6\u5019\u4f1a\u9047\u5230\u8fd9\u79cd\u60c5\u51b5 # \u901a\u8fc7time_features\u5bf9date\u6570\u636e\u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u5904\u7406 # data_stamp = time_features(df_stamp, timeenc=self.timeenc, freq=self.freq) # \u5bfc\u51fa\u6570\u636e self.data_x = data[border1:border2] self.data_y = data[border1:border2] self.data_stamp = data_stamp # \u6709\u65f6\u5019\u53ef\u80fd\u4f1a\u9047\u5230\u8fd9\u6837\u7684\u8bf4\u6cd5 self.data_x = data[border1:border2] #\u9700\u8981\u4f7f\u7528\u5f52\u4e00\u5316\u540e\u7684data if self.inverse: self.data_y = df_data.values[border1:border2] #\u4e0d\u9700\u8981\u5f52\u4e00\u5316\u540e\u7684\u6570\u636e else: self.data_y = data[border1:border2] #\u5982\u679cself.inverse\u662fFalse, data_x\u548cdata_y\u5c31\u5b8c\u5168\u76f8\u540c self.data_stamp = data_stamp \u7b2c\u4e09\u6b65\u662f\u83b7\u53d6\u6570\u636e getitem: \u9010\u4e2a\u8bfb\u53d6\u6837\u672c\u96c6\u5408\u7684\u5143\u7d20 # def __getitem__(self, index): s_begin = index #\u8d77\u59cb\u4e0b\u6807\u503c s_end = s_begin + self.seq_len #\u8f93\u5165\u7f16\u7801\u5668\u7684\u5e8f\u5217\u7684\u7ed3\u675f\u4e0b\u6807\u503c r_begin = s_end - self.label_len #start token\u8d77\u59cb\u4e0b\u6807\u503c r_end = r_begin + self.label_len + self.pred_len # \u89e3\u7801\u5668\u8f93\u51fa\u7684\u5e8f\u5217\u7ed3\u675f\u7684\u4e0b\u6807\u503c seq_x = self.data_x[s_begin:s_end] #\u8f93\u5165\u89e3\u7801\u5668\u7684\u5e8f\u5217 seq_y = self.data_y[r_begin:r_end] # \u89e3\u7801\u5668\u8f93\u51fa\u7684\u5e8f\u5217 seq_x_mark = self.data_stamp[s_begin:s_end] # start token\u7684\u5e8f\u5217,\u7ed9decoder\u7684 seq_y_mark = self.data_stamp[r_begin:r_end] # start token\u7684\u5e8f\u5217 return seq_x, seq_y, seq_x_mark, seq_y_mark \u7b2c\u56db\u6b65\u662f\u8fd4\u56de\u6837\u672c\u6570\u91cf\uff0c\u4ee5\u53ca\u8fdb\u884c\u53cd-\u5f52\u4e00\u5316 # def __len__(self): return len(self.data_x) - self.seq_len - self.pred_len + 1 def inverse_transform(self, data): return self.scaler.inverse_transform(data)","title":"DataLoader"},{"location":"Research/DataLoader/#dataloader","text":"","title":"DataLoader\u7684\u6784\u5efa"},{"location":"Research/DataLoader/#_1","text":"# \u8bfb\u53d6\u6587\u4ef6 import os # \u8fdb\u884c\u5fc5\u8981\u7684\u8ba1\u7b97 import numpy as np # \u753b\u56fe import pandas as pd # \u5bfc\u5165torch import torch # \u5bfc\u5165Dataset, \u8fd9\u91cc\u5176\u5b9e\u6211\u4eec\u9700\u8981\u81ea\u5b9a\u4e49 # \u5bfc\u5165DataLoader, \u7528\u6765\u6279\u6b21\u8bfb\u53d6\u6570\u636e from torch.utils.data import Dataset, DataLoader # \u5f52\u4e00\u5316\u5904\u7406\uff0c\u907f\u514d\u635f\u5931\u7684\u503c\u8fc7\u5927 from sklearn.preprocessing import StandardScaler # \u5bfc\u5165utils\u5f53\u4e2d\u4e3a\u628a\u65f6\u95f4\u6233\u5316\u4e3a\u7279\u5f81\u7684\u51fd\u6570 from utils.timefeatures import time_features # \u5ffd\u89c6\u635f\u5931 import warnings warnings.filterwarnings('ignore')","title":"\u5bfc\u5165\u5fc5\u8981\u7684\u5305"},{"location":"Research/DataLoader/#mydataset","text":"\u6bcf\u4e2a\u6570\u636e\u96c6\u9700\u8981\u6784\u5efa\u56db\u4e2a\u51fd\u6570 __init__(self, xxx,yyy): \u521d\u59cb\u5316\uff0c\u5bfc\u5165\u53c2\u6570 __read_data__(self): \u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4ee5\u53ca\u7279\u5f81\u5de5\u7a0b\u4ee5\u53ca\u5212\u5206\u6570\u636e\u96c6(\u9884\u5904\u7406\u7684\u90e8\u5206) __getitem__(self,index):\u7528\u4e8e\u9010\u4e2a\u8bfb\u53d6\u6837\u672c\u96c6\u5408\u4e2d\u7684\u5143\u7d20\uff0c\u53ef\u4ee5\u8fdb\u884c\u4e00\u5b9a\u7684\u53d8\u6362\uff0c\u5e76\u5c06\u8fd4\u56de\u6570\u636e\u96c6\u548c\u9a8c\u8bc1\u96c6\u6240\u9700\u8981\u7684\u6570\u636e __len__(self): \u8fd4\u56de\u6570\u636e\u96c6\u7684\u6837\u672c\u6570 __inverse_transform(self,data): \u8fd4\u56de\u53cd-\u5f52\u4e00\u5316\u7684\u6570\u636e class MyDataset(Dataset):","title":"\u4e3a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u6784\u9020\u4e0d\u540c\u7684\u6570\u636e\u96c6MyDataset, \u8fd9\u662f\u7531\u4e8e\u9700\u8981\u8bfb\u53d6\u4e0d\u540c\u7c7b\u578b\u7684\u6570\u636e\u96c6"},{"location":"Research/DataLoader/#_2","text":"def __init__(self, directory_path, Train_or_Test='train', size=None, features='S', data_name='Dataset_name.csv', predicted_col='OT', scale=True, inverse=False, timeenc=0, freq='h',cols=None) #def __init__(self, root_path, flag='train', size=None, # features='S', data_path='ETTh1.csv', # target='OT', scale=True, timeenc=0, freq='h', train_only=False): # size: # size[0]: \u4f20\u5230\u7f16\u7801\u5668\u5e8f\u5217\u7684\u957f\u5ea6 # size[1]: \u4f20\u5165\u5230\u89e3\u7801\u5668\u7684start token\u7684\u957f\u5ea6 # size[2]: \u9700\u8981\u9884\u6d4b\u7684\u5e8f\u5217\u957f\u5ea6 # \u628a\u7c7b\u578b\u75280\uff0c 1, 2\u8868\u793a assert flag in ['train', 'test', 'val'] type_map={'train':0, 'val':1, 'test':2} self.set_type=type_map[flag] # \u5bfc\u5165\u53c2\u6570 self.features =features self.predicted_col = predicted_col self.scale=scale self.inverse=inverse self.timeenc=timeenc self.freq=freq #\u6570\u636e\u7c92\u5ea6 self.root_path=root_path # \u5bfc\u5165\u6570\u636e\u96c6\u6240\u5728\u7684\u6587\u4ef6\u5939 self.data_path=data_path # \u5bfc\u5165\u6570\u636e\u96c6\u7684\u540d\u79f0 self.__read_data__()","title":"\u7b2c\u4e00\u6b65\uff1a\u521d\u59cb\u5316\u6709\u54ea\u4e9b\u53c2\u6570"},{"location":"Research/DataLoader/#_3","text":"__read_data_ def __read_data__(self): # \u5148\u51c6\u5907\u6570\u636e\u5f52\u4e00\u5316 self.scaler = StandardScaler() # \u8bfb\u53d6\u6570\u636e df_raw = pd.read_csv(os.path.join(self.root_path, self.data_path)) # \u73b0\u6839\u636e train, val, test\u6765\u628a\u5bf9\u5e94\u6570\u636e\u96c6\u5728\u539f\u59cb\u6570\u636e\u96c6\u7684index\u5212\u5206\u51fa\u6765 # {'train':0, 'val':1, 'test':2} border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len] border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24] # \u6839\u636e\u9884\u6d4b\u7684\u7c7b\u578b\u8fdb\u4e00\u6b65\u63d0\u53d6\u6570\u636e # M:\u591a\u5143\u9884\u6d4b\u591a\u5143 # S: \u5355\u5143\u9884\u6d4b\u5355\u5143 # MS: \u591a\u5143\u9884\u6d4b\u5355\u5143 if self.features == 'M' or self.features == 'MS': cols_data = df_raw.columns[1:] #\u56e0\u4e3a df_raw.columns[0]\u662fdate df_data = df_raw[cols_data] elif self.features == 'S': df_data = df_raw[[self.target]] # \u53ea\u7528\u5f85\u9884\u6d4b\u7684\u91cf\u6765\u9884\u6d4b\uff0c\u6240\u4ee5\u662ftarget # \u5982\u679c\u9700\u8981\u8fdb\u884c\u5f52\u4e00\u5316 # scale=None if self.scale: train_data = df_data[border1s[0]:border2s[0]] self.scaler.fit(train_data.values) data = self.scaler.transform(df_data.values) else: data = df_data.values # \u628a\u6570\u636e\u8f6c\u5316\u4e3anumpy\u6570\u7ec4 # \u53d6\u51fa\u5236\u5b9a\u8303\u56f4\u5185\u7684\u5e8f\u5217\u6570\u636e\u7684date\u5217 df_stamp = df_raw[['date']][border1:border2] df_stamp['date'] = pd.to_datetime(df_stamp.date) # \u5229\u7528 time_feature\u628adate\u6570\u636e\u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u5904\u7406 if self.timeenc == 0: df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1) df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1) df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1) df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1) data_stamp = df_stamp.drop(['date'], 1).values elif self.timeenc == 1: data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq) data_stamp = data_stamp.transpose(1, 0) # \u6709\u65f6\u5019\u4f1a\u9047\u5230\u8fd9\u79cd\u60c5\u51b5 # \u901a\u8fc7time_features\u5bf9date\u6570\u636e\u8fdb\u884c\u8fdb\u4e00\u6b65\u7684\u5904\u7406 # data_stamp = time_features(df_stamp, timeenc=self.timeenc, freq=self.freq) # \u5bfc\u51fa\u6570\u636e self.data_x = data[border1:border2] self.data_y = data[border1:border2] self.data_stamp = data_stamp # \u6709\u65f6\u5019\u53ef\u80fd\u4f1a\u9047\u5230\u8fd9\u6837\u7684\u8bf4\u6cd5 self.data_x = data[border1:border2] #\u9700\u8981\u4f7f\u7528\u5f52\u4e00\u5316\u540e\u7684data if self.inverse: self.data_y = df_data.values[border1:border2] #\u4e0d\u9700\u8981\u5f52\u4e00\u5316\u540e\u7684\u6570\u636e else: self.data_y = data[border1:border2] #\u5982\u679cself.inverse\u662fFalse, data_x\u548cdata_y\u5c31\u5b8c\u5168\u76f8\u540c self.data_stamp = data_stamp","title":"\u7b2c\u4e8c\u6b65\uff0c\u521d\u59cb\u5316\u65f6\u5019\u6700\u91cd\u8981\u7684\u51fd\u6570"},{"location":"Research/DataLoader/#getitem","text":"def __getitem__(self, index): s_begin = index #\u8d77\u59cb\u4e0b\u6807\u503c s_end = s_begin + self.seq_len #\u8f93\u5165\u7f16\u7801\u5668\u7684\u5e8f\u5217\u7684\u7ed3\u675f\u4e0b\u6807\u503c r_begin = s_end - self.label_len #start token\u8d77\u59cb\u4e0b\u6807\u503c r_end = r_begin + self.label_len + self.pred_len # \u89e3\u7801\u5668\u8f93\u51fa\u7684\u5e8f\u5217\u7ed3\u675f\u7684\u4e0b\u6807\u503c seq_x = self.data_x[s_begin:s_end] #\u8f93\u5165\u89e3\u7801\u5668\u7684\u5e8f\u5217 seq_y = self.data_y[r_begin:r_end] # \u89e3\u7801\u5668\u8f93\u51fa\u7684\u5e8f\u5217 seq_x_mark = self.data_stamp[s_begin:s_end] # start token\u7684\u5e8f\u5217,\u7ed9decoder\u7684 seq_y_mark = self.data_stamp[r_begin:r_end] # start token\u7684\u5e8f\u5217 return seq_x, seq_y, seq_x_mark, seq_y_mark","title":"\u7b2c\u4e09\u6b65\u662f\u83b7\u53d6\u6570\u636e getitem: \u9010\u4e2a\u8bfb\u53d6\u6837\u672c\u96c6\u5408\u7684\u5143\u7d20"},{"location":"Research/DataLoader/#-","text":"def __len__(self): return len(self.data_x) - self.seq_len - self.pred_len + 1 def inverse_transform(self, data): return self.scaler.inverse_transform(data)","title":"\u7b2c\u56db\u6b65\u662f\u8fd4\u56de\u6837\u672c\u6570\u91cf\uff0c\u4ee5\u53ca\u8fdb\u884c\u53cd-\u5f52\u4e00\u5316"},{"location":"Research/Day2/","text":"Paper 1 title: # Deterministic and Probabilistic Wind Power Forecasts by Considering Various Atmospheric Models and Feature Engineering Approaches Executive summary: The authors use three kind of numerical weather prediction (wind speed) rather than the single wind speed from the anemometer to serve as the feature engineering. The kernel of this paper is to use the specific data source to construct the feature and hence. What is useful for me is that we can consider the feature engineering in our work and cite this to alleviate the boring description . For reference, I think the following reference are of value: A New Fuzzy-Based Combined Prediction Interval for Wind Power Forecasting Technical description: PI coverage probability : show the percentage of the probability targets which will be covered by the upper and lower bound. \\[ \\mathrm{PICP}=\\frac{1}{N} \\sum_{t=1}^N c_t \\] where \\(N\\) is the number of samples and \\(c_t\\) is a Boolean value that is evaluated as follows: \\[ c_t= \\begin{cases}1, & y_t \\in\\left[L_t, U_t\\right] \\\\ 0, & y_t \\notin\\left[L_t, U_t\\right]\\end{cases} \\] where \\(y_t\\) is the forecast target and \\(U_t\\) and \\(L_t\\) are upper and lower bounds of the interval, respectively. PI normalized average width (PINAW): limit the high extra growth of the interval \\[ \\text { PINAW }=\\frac{1}{N R} \\sum_{t=1}^N\\left(U_t-L_t\\right) \\] where \\(R\\) is the range of the underlying targets used for normalizing PIs. The LUBE method could be regarded as a constrained nonlinear optimization problem with conflicting objective as follows: Objectives : \u200b Maximize: \\(\\operatorname{PICP}(w)\\) \u200b Minimize: PINAW \\((w)\\) Constraints : \\[ 0 \\leq \\operatorname{PICP}(w) \\leq 100 \\% \\] \u200b PINAW \\((w) \\succ 0\\) This is resolved by the following method: \\[ \\begin{aligned} & F(X)=\\min _{X \\in \\Omega}\\left\\{\\max _{i=1, \\ldots, n}\\left|\\mu_{\\text {ref }, i}-\\mu_{f, i}(X)\\right|\\right\\} \\\\ &=\\min _{X \\in \\Omega}\\left\\{\\operatorname { m a x } \\left(\\left|\\mu_{\\text {ref }, \\mathrm{PICP}}-\\mu_{\\mathrm{PICP}}(X)\\right|\\right.\\right. \\\\ &\\left.\\left.\\left|\\mu_{\\text {ref }, \\mathrm{PINAW}}-\\mu_{\\mathrm{PNAW}}(X)\\right|\\right)\\right\\} \\end{aligned} \\] where \\(n\\) is the number of objectives (here \\(n=2\\) ), \\(\\mu_{f, i}\\) is the membership function value of the \\(i\\) th objective, \\(\\Omega\\) is the problem search space; \\(X\\) is the control vector including the \\(\\mathrm{NN}\\) weighting factors, and \\(\\mu_{\\mathrm{ref}, i}\\) is the reference membership value for \\(i\\) -th objective. Paper 2 Title: # Privacy-preserving Spatiotemporal Scenario Generation of Renewable Energies: A Federated Deep Generative Learning Approach Executive summary: The authors want to use the federated learning with a central server to generate the scenarios for the wind power. The authors use the federated learning and the least square generative adversarial networks (LSGANs) for renewable scenario generation. What I think is useful for me is the concept of scenario generation and the application of federated learning. There are some references that I think is interesting: Technical description: Generative adversarial networks : GAN contains discriminator and generator, the generator is used to generated samples and the discriminator is used to judge the input data whether historical data or the generated data as much as possible. then the output of the discriminator network is \\[ \\left\\{\\begin{array}{l} p_{\\text {real }}=D(\\boldsymbol{x}) \\\\ p_{\\text {fake }}=D(G(\\boldsymbol{z})) \\end{array}\\right. \\] and the loss function of generated and discriminator are \\[ \\begin{gathered} L_G=\\mathbb{E}_{\\boldsymbol{z} \\sim P_Z}[\\log (1-D(G(\\boldsymbol{z})))] \\\\ L_D=-\\mathbb{E}_{\\boldsymbol{x} \\sim P_d}[\\log D(\\boldsymbol{x})]-\\mathbb{E}_{\\boldsymbol{z} \\sim P_d}[\\log (1-D(G(\\boldsymbol{z})))] \\end{gathered} \\] where \\(P_Z\\) is a known distribution that is easy to sample. Then the mini-max game model with value function \\(V_{\\mathrm{GANs}}(G, D)\\) is given by \\[ \\begin{aligned} \\min _G \\max _D V_{\\mathrm{GANs}}(G, D)= & \\mathbb{E}_{\\boldsymbol{x} \\sim P_d}[\\log D(\\boldsymbol{x})] \\\\ & +\\mathbb{E}_{\\boldsymbol{z} \\sim P_z}[\\log (1-D(G(\\boldsymbol{z})))] \\end{aligned} \\] Federated Learning: Suppose there are \\(N\\) clients, i.e. participating edge devices \\(\\left\\{\\mathcal{C}_1, \\mathcal{C}_2, \\ldots, \\mathcal{C}_N\\right\\}\\) based on their own dataset \\(\\left\\{\\mathcal{D}_1, \\mathcal{D}_2, \\ldots, \\mathcal{D}_N\\right\\}\\) traditional way: put all data together and train a big model Federated learning coordinates clients to train a global model \\(\\mathcal{M}_{\\mathrm{FED}}\\) deployed on a central server, not collecting all data. \\(\\delta\\) -accuracy loss : assuming that \\(\\mathcal{V}_{\\text {SUM }}\\) and \\(\\mathcal{V}_{\\mathrm{FED}}\\) are the performance metrics of the centralized model \\(\\mathcal{M}_{\\text {SUM }}\\) and federated model \\(\\mathcal{M}_{\\text {FED }}\\) , then \\(\\left|\\mathcal{V}_{\\mathrm{SUM}}-\\mathcal{V}_{\\mathrm{FED}}\\right|<\\delta\\) Global LSGANs Model : Traditionally, the generator is fixed, the optimal discriminator is as follows: \\[ D_{G, \\mathrm{GANs}}^*(\\boldsymbol{x})=\\frac{P_d(\\boldsymbol{x})}{P_d(\\boldsymbol{x})+P_g(\\boldsymbol{x})} \\] New: Substitute the above equation into \\[ \\begin{gathered} \\min _D V_{\\mathrm{LSGANs}}(D)=\\frac{1}{2} \\mathbb{E}_{\\boldsymbol{x} \\sim P_d}\\left[(D(\\boldsymbol{x})-b)^2\\right]+ \\\\ \\frac{1}{2} \\mathbb{E}_{\\boldsymbol{z} \\sim P_Z}\\left[(D(G(\\boldsymbol{z}))-a)^2\\right] \\\\ \\min _G V_{\\mathrm{LSGANs}}(G)=\\frac{1}{2} \\mathbb{E}_{\\boldsymbol{z} \\sim P_Z}\\left[(D(G(\\boldsymbol{z}))-c)^2\\right] \\end{gathered} \\] then we could get \\[ C_{\\mathrm{GANs}}(G)=V\\left(D_{D, \\mathrm{GANs}}^*, G\\right)=2 \\mathrm{JSD}\\left(P_d \\| P_g\\right)-\\log (4) \\] There are some drawbacks of the GAN, then least square-GAN are proposed. use \\(a-b\\) encoding and the least squares loss function, then the objective function of LSGAN is \\[ \\begin{gathered} \\min _D V_{\\mathrm{LSGANs}}(D)=\\frac{1}{2} \\mathbb{E}_{\\boldsymbol{x} \\sim P_d}\\left[(D(\\boldsymbol{x})-b)^2\\right]+ \\\\ \\frac{1}{2} \\mathbb{E}_{\\boldsymbol{z} \\sim P_Z}\\left[(D(G(\\boldsymbol{z}))-a)^2\\right] \\\\ \\min _G V_{\\mathrm{LSGANs}}(G)=\\frac{1}{2} \\mathbb{E}_{\\boldsymbol{z} \\sim P_Z}\\left[(D(G(\\boldsymbol{z}))-c)^2\\right] \\end{gathered} \\] The for the generator \\(G\\) , the optimal discriminator \\(D\\) is \\[ D_{G, \\text { LSGANs }}^*(\\boldsymbol{x})=\\frac{b P_d(\\boldsymbol{x})+a P_g(\\boldsymbol{x})}{P_d(\\boldsymbol{x})+P_g(\\boldsymbol{x})} \\] If we choose \\(b-c=1\\) and \\(b-a=2\\) , then we could get \\[ 2 C_{\\mathrm{LSGANs}}(G)=\\chi_{\\text {Pearson }}^2\\left(P_d+P_g \\| 2 P_g\\right) \\] where \\(\\chi_{\\text {Pearson }}^2\\) is the Pearson \\(\\chi^2\\) divergence. If \\(b-c=1\\) and \\(b-a=2\\) are satisfied, (8) is equivalent to minimize the Pearson \\(\\chi^2\\) divergence. network configuration: activation function, ReLU and LeakyReLU activation functions Then we consider the FederatedAveraging (FedAvg) algorithm. This algorithm is proposed in this paper: Communication-Efficient Learning of Deep Networks from Decentralized Data The major difference between federated optimization and distribution optimization. Non-IID: any particular user's local dataset will not be representative of the population distribution Unbalanced: Some users will make much heavier use of the service Massively distributed: Limited communication, Mobile devices are frequently offline or on slow or expensive connection. \\[ \\min _{w \\in \\mathbb{R}^d} f(w) \\quad \\text { where } \\quad f(w) \\stackrel{\\text { def }}{=} \\frac{1}{n} \\sum_{i=1}^n f_i(w) \\] For a machine learning problem, we typically take \\[ f_i(w)= \\ell\\left(x_i, y_i ; w\\right) \\] We assume that there are \\(K\\) clients and \\(\\mathcal{P}_k\\) the set of indexes of data points on client \\(k\\) , with \\(n_k = \\lvert \\mathcal{P}_k \\rvert\\) , then \\[ f(w)=\\sum_{k=1}^K \\frac{n_k}{n} F_k(w) \\quad \\text { where } \\quad F_k(w)=\\frac{1}{n_k} \\sum_{i \\in \\mathcal{P}_k} f_i(w) \\text {. } \\] three key parameters: \\(C\\) , the fraction of clients that perform computation on each round; \\(E\\) , then number of training passes each client makes over its local dataset on each round; and \\(B\\) , the local minibatch size used for the client updates. We write \\(B=\\infty\\) to indicate that the full data are used. \\[ \\begin{aligned} & \\hline \\text { Algorithm } 1 \\text { FederatedAveraging. The } K \\text { clients are } \\\\ & \\text { indexed by } k ; B \\text { is the local minibatch size, } E \\text { is the number } \\\\ & \\text { of local epochs, and } \\eta \\text { is the learning rate. } \\\\ & \\hline \\text { Server executes: } \\\\ & \\text { initialize } w_0 \\\\ & \\text { for each round } t=1,2, \\ldots \\text { do } \\\\ & \\quad m \\leftarrow \\max (C \\cdot K, 1) \\\\ & \\quad S_t \\leftarrow(\\operatorname{random} \\text { set of } m \\text { clients) } \\\\ & \\quad \\text { for each client } k \\in S_t \\text { in parallel do } \\\\ & \\quad w_{t+1}^k \\leftarrow \\text { ClientUpdate }\\left(k, w_t\\right) \\\\ & \\quad w_{t+1} \\leftarrow \\sum_{k=1}^K \\frac{n_k}{n} w_{t+1}^k \\\\ & \\text { ClientUpdate }(k, w): / / \\text { Run on client } k \\\\ & \\mathcal{B} \\leftarrow\\left(\\text { split } \\mathcal{P}_k \\text { into batches of size } B\\right) \\\\ & \\text { for each local epoch } i \\text { from } 1 \\text { to } E \\text { do } \\\\ & \\text { for batch } b \\in \\mathcal{B} \\text { do } \\\\ & \\quad w \\leftarrow w-\\eta \\nabla \\ell(w ; b) \\\\ & \\text { return } w \\text { to server } \\end{aligned} \\] Then the algorithm could be (there is no contribution, except the GANs optimization part.) Correlation Analysis: \\[ R(\\tau)=\\frac{\\mathbb{E}\\left[\\left(S_t-\\mu\\right)\\left(S_{t+\\tau}-\\mu\\right)\\right]}{\\sigma^2} \\] where \\(S\\) is a random time series; \\(\\mu\\) and \\(\\sigma\\) denote the mean and variance of \\(S\\) , respectively; and \\(\\tau\\) is the time lag. We use the continuous ranked probability score ( CRPS ) which measures the dissimilarity of the cumulative distributions between generated scenarios and historical observations. The score at lead time \\(\\ell\\) is defined as \\[ \\operatorname{CRPS}_l=\\frac{1}{M} \\sum_{t=1}^M \\int_0^1\\left(\\widehat{F}_{t+l \\mid t}(\\xi)-\\mathbf{1}\\left(\\xi \\geq \\xi_{t+l}\\right)\\right)^2 d \\xi \\] where \\(M\\) is the total number of scenarios, \\(\\widehat{F}_{t+l \\mid t}(\\xi)\\) denotes the cumulative distribution function of normalized scenario, and \\(\\mathbf{1}\\left(\\xi \\geq \\xi_{t+l}\\right)\\) is the indicator function for comparing scenarios and observation. Fr\u00e9chet inception Distance (FID) \\[ \\operatorname{FID}\\left(P_d, P_g\\right)=\\left\\|\\mu_d-\\mu_g\\right\\|+\\operatorname{Tr}\\left(\\Sigma_d+\\Sigma_g-2\\left(\\Sigma_d \\Sigma_g\\right)^{\\frac{1}{2}}\\right) \\] where \\(\\mu_d\\) and \\(\\mu_g\\) represent the empirical mean; \\(\\Sigma_d\\) and \\(\\Sigma_g\\) are empirical covariance. Kernel Maximum Mean Discrepancy (MMD): measures the difference between \\(P_d\\) and \\(P_g\\) for some fixed kernel function \\(k\\) , which is defined as \\[ \\operatorname{MMD}^2\\left(P_d, P_g\\right)=\\underset{\\substack{x, x^{\\prime} \\sim P_d \\\\ y, y^{\\prime} \\sim P_g}}{ }\\left[k\\left(x, x^{\\prime}\\right)-2 k(x, y)+k\\left(y, y^{\\prime}\\right)\\right] \\] The 1-Nearest Neighbor classifier Energy Score (ES) \\[ \\mathrm{ES}=\\frac{1}{M} \\sum_{i=1}^M\\left\\|\\varsigma-\\xi_i\\right\\|-\\frac{1}{2 M^2} \\sum_{i=1}^M \\sum_{j=1}^M\\left\\|\\xi_i-\\xi_j\\right\\| \\] \\(\\varsigma\\) is the real renewable power output, \\(\\xi_i\\) is the \\(i\\) -th generated time series scenario and \\(M\\) denotes the number of scenarios. Pearson correlation coefficient \\(\\rho\\) of two time series \\(S_i\\) and \\(S_j\\) is \\[ \\rho\\left(S_i, S_j\\right)=\\frac{\\sum_{i=1}^n\\left(S_i-\\bar{S}_i\\right)\\left(S_j-\\bar{S}_j\\right)}{\\sqrt{\\sum_{i=1}^n\\left(S_i-\\bar{S}_i\\right)^2} \\sqrt{\\sum_{i=1}^n\\left(S_j-\\bar{S}_j\\right)^2}} \\]","title":"Day 2"},{"location":"Research/Day2/#paper-1-title","text":"Deterministic and Probabilistic Wind Power Forecasts by Considering Various Atmospheric Models and Feature Engineering Approaches Executive summary: The authors use three kind of numerical weather prediction (wind speed) rather than the single wind speed from the anemometer to serve as the feature engineering. The kernel of this paper is to use the specific data source to construct the feature and hence. What is useful for me is that we can consider the feature engineering in our work and cite this to alleviate the boring description . For reference, I think the following reference are of value: A New Fuzzy-Based Combined Prediction Interval for Wind Power Forecasting Technical description: PI coverage probability : show the percentage of the probability targets which will be covered by the upper and lower bound. \\[ \\mathrm{PICP}=\\frac{1}{N} \\sum_{t=1}^N c_t \\] where \\(N\\) is the number of samples and \\(c_t\\) is a Boolean value that is evaluated as follows: \\[ c_t= \\begin{cases}1, & y_t \\in\\left[L_t, U_t\\right] \\\\ 0, & y_t \\notin\\left[L_t, U_t\\right]\\end{cases} \\] where \\(y_t\\) is the forecast target and \\(U_t\\) and \\(L_t\\) are upper and lower bounds of the interval, respectively. PI normalized average width (PINAW): limit the high extra growth of the interval \\[ \\text { PINAW }=\\frac{1}{N R} \\sum_{t=1}^N\\left(U_t-L_t\\right) \\] where \\(R\\) is the range of the underlying targets used for normalizing PIs. The LUBE method could be regarded as a constrained nonlinear optimization problem with conflicting objective as follows: Objectives : \u200b Maximize: \\(\\operatorname{PICP}(w)\\) \u200b Minimize: PINAW \\((w)\\) Constraints : \\[ 0 \\leq \\operatorname{PICP}(w) \\leq 100 \\% \\] \u200b PINAW \\((w) \\succ 0\\) This is resolved by the following method: \\[ \\begin{aligned} & F(X)=\\min _{X \\in \\Omega}\\left\\{\\max _{i=1, \\ldots, n}\\left|\\mu_{\\text {ref }, i}-\\mu_{f, i}(X)\\right|\\right\\} \\\\ &=\\min _{X \\in \\Omega}\\left\\{\\operatorname { m a x } \\left(\\left|\\mu_{\\text {ref }, \\mathrm{PICP}}-\\mu_{\\mathrm{PICP}}(X)\\right|\\right.\\right. \\\\ &\\left.\\left.\\left|\\mu_{\\text {ref }, \\mathrm{PINAW}}-\\mu_{\\mathrm{PNAW}}(X)\\right|\\right)\\right\\} \\end{aligned} \\] where \\(n\\) is the number of objectives (here \\(n=2\\) ), \\(\\mu_{f, i}\\) is the membership function value of the \\(i\\) th objective, \\(\\Omega\\) is the problem search space; \\(X\\) is the control vector including the \\(\\mathrm{NN}\\) weighting factors, and \\(\\mu_{\\mathrm{ref}, i}\\) is the reference membership value for \\(i\\) -th objective.","title":"Paper 1 title:"},{"location":"Research/Day2/#paper-2-title","text":"Privacy-preserving Spatiotemporal Scenario Generation of Renewable Energies: A Federated Deep Generative Learning Approach Executive summary: The authors want to use the federated learning with a central server to generate the scenarios for the wind power. The authors use the federated learning and the least square generative adversarial networks (LSGANs) for renewable scenario generation. What I think is useful for me is the concept of scenario generation and the application of federated learning. There are some references that I think is interesting: Technical description: Generative adversarial networks : GAN contains discriminator and generator, the generator is used to generated samples and the discriminator is used to judge the input data whether historical data or the generated data as much as possible. then the output of the discriminator network is \\[ \\left\\{\\begin{array}{l} p_{\\text {real }}=D(\\boldsymbol{x}) \\\\ p_{\\text {fake }}=D(G(\\boldsymbol{z})) \\end{array}\\right. \\] and the loss function of generated and discriminator are \\[ \\begin{gathered} L_G=\\mathbb{E}_{\\boldsymbol{z} \\sim P_Z}[\\log (1-D(G(\\boldsymbol{z})))] \\\\ L_D=-\\mathbb{E}_{\\boldsymbol{x} \\sim P_d}[\\log D(\\boldsymbol{x})]-\\mathbb{E}_{\\boldsymbol{z} \\sim P_d}[\\log (1-D(G(\\boldsymbol{z})))] \\end{gathered} \\] where \\(P_Z\\) is a known distribution that is easy to sample. Then the mini-max game model with value function \\(V_{\\mathrm{GANs}}(G, D)\\) is given by \\[ \\begin{aligned} \\min _G \\max _D V_{\\mathrm{GANs}}(G, D)= & \\mathbb{E}_{\\boldsymbol{x} \\sim P_d}[\\log D(\\boldsymbol{x})] \\\\ & +\\mathbb{E}_{\\boldsymbol{z} \\sim P_z}[\\log (1-D(G(\\boldsymbol{z})))] \\end{aligned} \\] Federated Learning: Suppose there are \\(N\\) clients, i.e. participating edge devices \\(\\left\\{\\mathcal{C}_1, \\mathcal{C}_2, \\ldots, \\mathcal{C}_N\\right\\}\\) based on their own dataset \\(\\left\\{\\mathcal{D}_1, \\mathcal{D}_2, \\ldots, \\mathcal{D}_N\\right\\}\\) traditional way: put all data together and train a big model Federated learning coordinates clients to train a global model \\(\\mathcal{M}_{\\mathrm{FED}}\\) deployed on a central server, not collecting all data. \\(\\delta\\) -accuracy loss : assuming that \\(\\mathcal{V}_{\\text {SUM }}\\) and \\(\\mathcal{V}_{\\mathrm{FED}}\\) are the performance metrics of the centralized model \\(\\mathcal{M}_{\\text {SUM }}\\) and federated model \\(\\mathcal{M}_{\\text {FED }}\\) , then \\(\\left|\\mathcal{V}_{\\mathrm{SUM}}-\\mathcal{V}_{\\mathrm{FED}}\\right|<\\delta\\) Global LSGANs Model : Traditionally, the generator is fixed, the optimal discriminator is as follows: \\[ D_{G, \\mathrm{GANs}}^*(\\boldsymbol{x})=\\frac{P_d(\\boldsymbol{x})}{P_d(\\boldsymbol{x})+P_g(\\boldsymbol{x})} \\] New: Substitute the above equation into \\[ \\begin{gathered} \\min _D V_{\\mathrm{LSGANs}}(D)=\\frac{1}{2} \\mathbb{E}_{\\boldsymbol{x} \\sim P_d}\\left[(D(\\boldsymbol{x})-b)^2\\right]+ \\\\ \\frac{1}{2} \\mathbb{E}_{\\boldsymbol{z} \\sim P_Z}\\left[(D(G(\\boldsymbol{z}))-a)^2\\right] \\\\ \\min _G V_{\\mathrm{LSGANs}}(G)=\\frac{1}{2} \\mathbb{E}_{\\boldsymbol{z} \\sim P_Z}\\left[(D(G(\\boldsymbol{z}))-c)^2\\right] \\end{gathered} \\] then we could get \\[ C_{\\mathrm{GANs}}(G)=V\\left(D_{D, \\mathrm{GANs}}^*, G\\right)=2 \\mathrm{JSD}\\left(P_d \\| P_g\\right)-\\log (4) \\] There are some drawbacks of the GAN, then least square-GAN are proposed. use \\(a-b\\) encoding and the least squares loss function, then the objective function of LSGAN is \\[ \\begin{gathered} \\min _D V_{\\mathrm{LSGANs}}(D)=\\frac{1}{2} \\mathbb{E}_{\\boldsymbol{x} \\sim P_d}\\left[(D(\\boldsymbol{x})-b)^2\\right]+ \\\\ \\frac{1}{2} \\mathbb{E}_{\\boldsymbol{z} \\sim P_Z}\\left[(D(G(\\boldsymbol{z}))-a)^2\\right] \\\\ \\min _G V_{\\mathrm{LSGANs}}(G)=\\frac{1}{2} \\mathbb{E}_{\\boldsymbol{z} \\sim P_Z}\\left[(D(G(\\boldsymbol{z}))-c)^2\\right] \\end{gathered} \\] The for the generator \\(G\\) , the optimal discriminator \\(D\\) is \\[ D_{G, \\text { LSGANs }}^*(\\boldsymbol{x})=\\frac{b P_d(\\boldsymbol{x})+a P_g(\\boldsymbol{x})}{P_d(\\boldsymbol{x})+P_g(\\boldsymbol{x})} \\] If we choose \\(b-c=1\\) and \\(b-a=2\\) , then we could get \\[ 2 C_{\\mathrm{LSGANs}}(G)=\\chi_{\\text {Pearson }}^2\\left(P_d+P_g \\| 2 P_g\\right) \\] where \\(\\chi_{\\text {Pearson }}^2\\) is the Pearson \\(\\chi^2\\) divergence. If \\(b-c=1\\) and \\(b-a=2\\) are satisfied, (8) is equivalent to minimize the Pearson \\(\\chi^2\\) divergence. network configuration: activation function, ReLU and LeakyReLU activation functions Then we consider the FederatedAveraging (FedAvg) algorithm. This algorithm is proposed in this paper: Communication-Efficient Learning of Deep Networks from Decentralized Data The major difference between federated optimization and distribution optimization. Non-IID: any particular user's local dataset will not be representative of the population distribution Unbalanced: Some users will make much heavier use of the service Massively distributed: Limited communication, Mobile devices are frequently offline or on slow or expensive connection. \\[ \\min _{w \\in \\mathbb{R}^d} f(w) \\quad \\text { where } \\quad f(w) \\stackrel{\\text { def }}{=} \\frac{1}{n} \\sum_{i=1}^n f_i(w) \\] For a machine learning problem, we typically take \\[ f_i(w)= \\ell\\left(x_i, y_i ; w\\right) \\] We assume that there are \\(K\\) clients and \\(\\mathcal{P}_k\\) the set of indexes of data points on client \\(k\\) , with \\(n_k = \\lvert \\mathcal{P}_k \\rvert\\) , then \\[ f(w)=\\sum_{k=1}^K \\frac{n_k}{n} F_k(w) \\quad \\text { where } \\quad F_k(w)=\\frac{1}{n_k} \\sum_{i \\in \\mathcal{P}_k} f_i(w) \\text {. } \\] three key parameters: \\(C\\) , the fraction of clients that perform computation on each round; \\(E\\) , then number of training passes each client makes over its local dataset on each round; and \\(B\\) , the local minibatch size used for the client updates. We write \\(B=\\infty\\) to indicate that the full data are used. \\[ \\begin{aligned} & \\hline \\text { Algorithm } 1 \\text { FederatedAveraging. The } K \\text { clients are } \\\\ & \\text { indexed by } k ; B \\text { is the local minibatch size, } E \\text { is the number } \\\\ & \\text { of local epochs, and } \\eta \\text { is the learning rate. } \\\\ & \\hline \\text { Server executes: } \\\\ & \\text { initialize } w_0 \\\\ & \\text { for each round } t=1,2, \\ldots \\text { do } \\\\ & \\quad m \\leftarrow \\max (C \\cdot K, 1) \\\\ & \\quad S_t \\leftarrow(\\operatorname{random} \\text { set of } m \\text { clients) } \\\\ & \\quad \\text { for each client } k \\in S_t \\text { in parallel do } \\\\ & \\quad w_{t+1}^k \\leftarrow \\text { ClientUpdate }\\left(k, w_t\\right) \\\\ & \\quad w_{t+1} \\leftarrow \\sum_{k=1}^K \\frac{n_k}{n} w_{t+1}^k \\\\ & \\text { ClientUpdate }(k, w): / / \\text { Run on client } k \\\\ & \\mathcal{B} \\leftarrow\\left(\\text { split } \\mathcal{P}_k \\text { into batches of size } B\\right) \\\\ & \\text { for each local epoch } i \\text { from } 1 \\text { to } E \\text { do } \\\\ & \\text { for batch } b \\in \\mathcal{B} \\text { do } \\\\ & \\quad w \\leftarrow w-\\eta \\nabla \\ell(w ; b) \\\\ & \\text { return } w \\text { to server } \\end{aligned} \\] Then the algorithm could be (there is no contribution, except the GANs optimization part.) Correlation Analysis: \\[ R(\\tau)=\\frac{\\mathbb{E}\\left[\\left(S_t-\\mu\\right)\\left(S_{t+\\tau}-\\mu\\right)\\right]}{\\sigma^2} \\] where \\(S\\) is a random time series; \\(\\mu\\) and \\(\\sigma\\) denote the mean and variance of \\(S\\) , respectively; and \\(\\tau\\) is the time lag. We use the continuous ranked probability score ( CRPS ) which measures the dissimilarity of the cumulative distributions between generated scenarios and historical observations. The score at lead time \\(\\ell\\) is defined as \\[ \\operatorname{CRPS}_l=\\frac{1}{M} \\sum_{t=1}^M \\int_0^1\\left(\\widehat{F}_{t+l \\mid t}(\\xi)-\\mathbf{1}\\left(\\xi \\geq \\xi_{t+l}\\right)\\right)^2 d \\xi \\] where \\(M\\) is the total number of scenarios, \\(\\widehat{F}_{t+l \\mid t}(\\xi)\\) denotes the cumulative distribution function of normalized scenario, and \\(\\mathbf{1}\\left(\\xi \\geq \\xi_{t+l}\\right)\\) is the indicator function for comparing scenarios and observation. Fr\u00e9chet inception Distance (FID) \\[ \\operatorname{FID}\\left(P_d, P_g\\right)=\\left\\|\\mu_d-\\mu_g\\right\\|+\\operatorname{Tr}\\left(\\Sigma_d+\\Sigma_g-2\\left(\\Sigma_d \\Sigma_g\\right)^{\\frac{1}{2}}\\right) \\] where \\(\\mu_d\\) and \\(\\mu_g\\) represent the empirical mean; \\(\\Sigma_d\\) and \\(\\Sigma_g\\) are empirical covariance. Kernel Maximum Mean Discrepancy (MMD): measures the difference between \\(P_d\\) and \\(P_g\\) for some fixed kernel function \\(k\\) , which is defined as \\[ \\operatorname{MMD}^2\\left(P_d, P_g\\right)=\\underset{\\substack{x, x^{\\prime} \\sim P_d \\\\ y, y^{\\prime} \\sim P_g}}{ }\\left[k\\left(x, x^{\\prime}\\right)-2 k(x, y)+k\\left(y, y^{\\prime}\\right)\\right] \\] The 1-Nearest Neighbor classifier Energy Score (ES) \\[ \\mathrm{ES}=\\frac{1}{M} \\sum_{i=1}^M\\left\\|\\varsigma-\\xi_i\\right\\|-\\frac{1}{2 M^2} \\sum_{i=1}^M \\sum_{j=1}^M\\left\\|\\xi_i-\\xi_j\\right\\| \\] \\(\\varsigma\\) is the real renewable power output, \\(\\xi_i\\) is the \\(i\\) -th generated time series scenario and \\(M\\) denotes the number of scenarios. Pearson correlation coefficient \\(\\rho\\) of two time series \\(S_i\\) and \\(S_j\\) is \\[ \\rho\\left(S_i, S_j\\right)=\\frac{\\sum_{i=1}^n\\left(S_i-\\bar{S}_i\\right)\\left(S_j-\\bar{S}_j\\right)}{\\sqrt{\\sum_{i=1}^n\\left(S_i-\\bar{S}_i\\right)^2} \\sqrt{\\sum_{i=1}^n\\left(S_j-\\bar{S}_j\\right)^2}} \\]","title":"Paper 2 Title:"},{"location":"Research/Linear_Regression/","text":"Linear Regression # Linear Regression\u7ebf\u6027\u56de\u5f52\uff0c\u7b97\u662f\u4e00\u79cd\u7b80\u5355\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u65b9\u6cd5\uff0c\u8fd9\u91cc\u6211\u4f7f\u7528pytorch\u53bb\u5b9e\u73b0\u8fd9\u7c7b\u7b97\u6cd5\u3002 \u57fa\u672c\u539f\u7406 # Assume our prediction \\(\\hat{z}_t\\) is a weighted combination of features \\(x_{t, 1}, \\ldots, x_{t, D}\\) , $$ \\widehat{z} t=\\sum {d=1}^D w_d x_{t, d} $$ The features \\(x_{t, d}\\) are assumed to be given (= hand designed) Goal: find weights \\(w_1, \\ldots, w_D\\) so that our prediction \\(\\hat{z}_t\\) is close to the true \\(z_t\\) . Here we use Least square method # \\(w^{\\star}=\\underset{w}{\\operatorname{argmin}} \\sum_{t=1}^T\\left(z_t-\\widehat{z}_t\\right)^2=\\sum_{t=1}^T\\left(z_t-\\sum_{d=1}^D w_d x_{t, d}\\right)^2\\) Then we can use this to make forecasts: $$ z_t=\\sum_{d=1}^D w_d^{\\star} x_{t, d} \\quad t=T+1, \\ldots, T+h $$ \u6ce8\u610f\u8fd9\u5e76\u4e0d\u662f\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u6240\u4ee5\u4e0d\u9700\u8981Dataloader \u8fd9\u91cc\u7ecf\u5e38\u4f7f\u7528\u7684\u7279\u5f81 $$ x_{1, d}, x_{2, d}, \\ldots, x_{T+H, d} $$ \u6709\u5982\u4e0b\u7684\u7279\u5f81\uff1a Trend Feature (linear, logarithmic, exponential, logistic) Seasonal features: dummies (one-hot indicators), periodic features (Fourier, wavelet, etc) Lagged target value (e.g use \\(z_{t-1}\\) and \\(z_{t-2}\\) as features to predict \\(z_t\\) ) Seasonal lagged target values (e.g use \\(z_{t-S}\\) to predict \\(z_t\\) , with \\(S=12\\) for monthly data ) (Weighted) average features (e.g. \\(\\operatorname{mean}\\left(z_{t-7: t-1}\\right)\\) ) \u4ee3\u7801\u5b9e\u73b0\uff1a import torch import numpy as np import matplotlib.pyplot as plt X = torch.arange(-5, 5, 0.1).view(-1, 1) func = -5 * X Y = func + 0.4 * torch.randn(X.size()) # defining the function for forward pass for prediction def forward(x): return w * x # evaluating data points with Mean Square Error def criterion(y_pred, y): return torch.mean((y_pred - y) ** 2) w = torch.tensor(-10.0, requires_grad=True) step_size = 0.1 loss_list = [] iter = 20 for i in range (iter): # making predictions with forward pass Y_pred = forward(X) # calculating the loss between original and predicted data points loss = criterion(Y_pred, Y) # storing the calculated loss in a list loss_list.append(loss.item()) # backward pass for computing the gradients of the loss w.r.t to learnable parameters loss.backward() # updateing the parameters after each iteration w.data = w.data - step_size * w.grad.data # zeroing gradients after each iteration w.grad.data.zero_() # priting the values for understanding print('{},\\t{},\\t{}'.format(i, loss.item(), w.item())) # Plotting the loss after each iteration plt.plot(loss_list, 'r') plt.tight_layout() plt.grid('True', color='y') plt.xlabel(\"Epochs/Iterations\") plt.ylabel(\"Loss\") plt.show() \u5982\u679c\u8003\u8651\u504f\u7f6e $$ \\widehat{z} t=\\sum {d=1}^D w_d x_{t, d}+b $$ \u5219\u6b64\u65f6\u6211\u4eec\u9700\u8981\u5bf9 \\(w\\) , \\(b\\) \u5206\u522b\u66f4\u65b0\u6b65\u957f\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a import torch import numpy as np import matplotlib.pyplot as plt X = torch.arange(-5, 5, 0.1).view(-1, 1) func = -5 * X Y = func + 0.4 * torch.randn(X.size()) # defining the function for forward pass for prediction def forward(x): return w * x + b # evaluating data points with Mean Square Error. def criterion(y_pred, y): return torch.mean((y_pred - y) ** 2) w = torch.tensor(-10.0, requires_grad=True) b = torch.tensor(-20.0, requires_grad=True) step_size = 0.1 loss_list = [] iter = 20 for i in range (iter): # making predictions with forward pass Y_pred = forward(X) # calculating the loss between original and predicted data points loss = criterion(Y_pred, Y) # storing the calculated loss in a list loss_list.append(loss.item()) # backward pass for computing the gradients of the loss w.r.t to learnable parameters loss.backward() # updateing the parameters after each iteration w.data = w.data - step_size * w.grad.data b.data = b.data - step_size * b.grad.data # zeroing gradients after each iteration w.grad.data.zero_() b.grad.data.zero_() # priting the values for understanding print('{}, \\t{}, \\t{}, \\t{}'.format(i, loss.item(), w.item(), b.item())) # Plotting the loss after each iteration plt.plot(loss_list, 'r') plt.tight_layout() plt.grid('True', color='y') plt.xlabel(\"Epochs/Iterations\") plt.ylabel(\"Loss\") plt.show()","title":"Linear Regression"},{"location":"Research/Linear_Regression/#linear-regression","text":"Linear Regression\u7ebf\u6027\u56de\u5f52\uff0c\u7b97\u662f\u4e00\u79cd\u7b80\u5355\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u65b9\u6cd5\uff0c\u8fd9\u91cc\u6211\u4f7f\u7528pytorch\u53bb\u5b9e\u73b0\u8fd9\u7c7b\u7b97\u6cd5\u3002","title":"Linear Regression"},{"location":"Research/Linear_Regression/#_1","text":"Assume our prediction \\(\\hat{z}_t\\) is a weighted combination of features \\(x_{t, 1}, \\ldots, x_{t, D}\\) , $$ \\widehat{z} t=\\sum {d=1}^D w_d x_{t, d} $$ The features \\(x_{t, d}\\) are assumed to be given (= hand designed)","title":"\u57fa\u672c\u539f\u7406"},{"location":"Research/Linear_Regression/#goal-find-weights-w_1-ldots-w_d-so-that-our-prediction-hatz_t-is-close-to-the-true-z_t-here-we-use-least-square-method","text":"\\(w^{\\star}=\\underset{w}{\\operatorname{argmin}} \\sum_{t=1}^T\\left(z_t-\\widehat{z}_t\\right)^2=\\sum_{t=1}^T\\left(z_t-\\sum_{d=1}^D w_d x_{t, d}\\right)^2\\) Then we can use this to make forecasts: $$ z_t=\\sum_{d=1}^D w_d^{\\star} x_{t, d} \\quad t=T+1, \\ldots, T+h $$ \u6ce8\u610f\u8fd9\u5e76\u4e0d\u662f\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u6240\u4ee5\u4e0d\u9700\u8981Dataloader \u8fd9\u91cc\u7ecf\u5e38\u4f7f\u7528\u7684\u7279\u5f81 $$ x_{1, d}, x_{2, d}, \\ldots, x_{T+H, d} $$ \u6709\u5982\u4e0b\u7684\u7279\u5f81\uff1a Trend Feature (linear, logarithmic, exponential, logistic) Seasonal features: dummies (one-hot indicators), periodic features (Fourier, wavelet, etc) Lagged target value (e.g use \\(z_{t-1}\\) and \\(z_{t-2}\\) as features to predict \\(z_t\\) ) Seasonal lagged target values (e.g use \\(z_{t-S}\\) to predict \\(z_t\\) , with \\(S=12\\) for monthly data ) (Weighted) average features (e.g. \\(\\operatorname{mean}\\left(z_{t-7: t-1}\\right)\\) ) \u4ee3\u7801\u5b9e\u73b0\uff1a import torch import numpy as np import matplotlib.pyplot as plt X = torch.arange(-5, 5, 0.1).view(-1, 1) func = -5 * X Y = func + 0.4 * torch.randn(X.size()) # defining the function for forward pass for prediction def forward(x): return w * x # evaluating data points with Mean Square Error def criterion(y_pred, y): return torch.mean((y_pred - y) ** 2) w = torch.tensor(-10.0, requires_grad=True) step_size = 0.1 loss_list = [] iter = 20 for i in range (iter): # making predictions with forward pass Y_pred = forward(X) # calculating the loss between original and predicted data points loss = criterion(Y_pred, Y) # storing the calculated loss in a list loss_list.append(loss.item()) # backward pass for computing the gradients of the loss w.r.t to learnable parameters loss.backward() # updateing the parameters after each iteration w.data = w.data - step_size * w.grad.data # zeroing gradients after each iteration w.grad.data.zero_() # priting the values for understanding print('{},\\t{},\\t{}'.format(i, loss.item(), w.item())) # Plotting the loss after each iteration plt.plot(loss_list, 'r') plt.tight_layout() plt.grid('True', color='y') plt.xlabel(\"Epochs/Iterations\") plt.ylabel(\"Loss\") plt.show() \u5982\u679c\u8003\u8651\u504f\u7f6e $$ \\widehat{z} t=\\sum {d=1}^D w_d x_{t, d}+b $$ \u5219\u6b64\u65f6\u6211\u4eec\u9700\u8981\u5bf9 \\(w\\) , \\(b\\) \u5206\u522b\u66f4\u65b0\u6b65\u957f\uff0c\u4ee3\u7801\u5982\u4e0b\uff1a import torch import numpy as np import matplotlib.pyplot as plt X = torch.arange(-5, 5, 0.1).view(-1, 1) func = -5 * X Y = func + 0.4 * torch.randn(X.size()) # defining the function for forward pass for prediction def forward(x): return w * x + b # evaluating data points with Mean Square Error. def criterion(y_pred, y): return torch.mean((y_pred - y) ** 2) w = torch.tensor(-10.0, requires_grad=True) b = torch.tensor(-20.0, requires_grad=True) step_size = 0.1 loss_list = [] iter = 20 for i in range (iter): # making predictions with forward pass Y_pred = forward(X) # calculating the loss between original and predicted data points loss = criterion(Y_pred, Y) # storing the calculated loss in a list loss_list.append(loss.item()) # backward pass for computing the gradients of the loss w.r.t to learnable parameters loss.backward() # updateing the parameters after each iteration w.data = w.data - step_size * w.grad.data b.data = b.data - step_size * b.grad.data # zeroing gradients after each iteration w.grad.data.zero_() b.grad.data.zero_() # priting the values for understanding print('{}, \\t{}, \\t{}, \\t{}'.format(i, loss.item(), w.item(), b.item())) # Plotting the loss after each iteration plt.plot(loss_list, 'r') plt.tight_layout() plt.grid('True', color='y') plt.xlabel(\"Epochs/Iterations\") plt.ylabel(\"Loss\") plt.show()","title":"Goal: find weights \\(w_1, \\ldots, w_D\\) so that our prediction \\(\\hat{z}_t\\) is close to the true \\(z_t\\). Here we use Least square method"},{"location":"Research/Multivariate_EDA/","text":"Multivariate Exploratory Data Analyses\uff08EDA) # Exploratory Data Analyses (EDA) \u53eb\u505a\u6570\u636e\u63a2\u7d22\u6027\u5206\u6790\uff0c\u662f\u7528\u6765\u5728\u5bf9\u65f6\u95f4\u5e8f\u5217\u5904\u7406\u524d\u5148\u8fdb\u884c\u6570\u636e \u5355\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u662f\u6709\u591a\u4e2a\u53d8\u91cf\u7684\uff08\u9664\u65f6\u95f4\u5916\uff09\u7684\u5e8f\u5217\uff0c\u4e0b\u9762\u6211\u4eec\u8003\u8651\u5355\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u7684EDA\u95ee\u9898 \u5bfc\u5165\u65f6\u95f4\u5e8f\u5217 # import pandas as pd df=pd.read_csv('TexasTurbine.csv') # \u6253\u5370\u524d\u4e8c\u5341\u884c df.head(20) \u5bfc\u5165\u5fc5\u8981\u7684\u5305 # import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt plt.style.use(\"Solarize_Light2\") #using style ggplot %matplotlib inline from mpl_toolkits.mplot3d import Axes3D import datetime as dt import plotly.graph_objects as go import plotly.express as px import datetime as dt \u83b7\u53d6\u6570\u636e\u70b9\u7684\u6570\u91cf(\u6570\u636e\u63cf\u8ff0\u7684\u65f6\u5019\u4f7f\u7528) # df.shape \u83b7\u53d6\u6570\u636e\u7684\u7c7b\u578b # df.info() \u628aTimestamp\u8f6c\u5316\u4e3astrftime\u7c7b\u578b # df['Time stamp'] = pd.to_datetime(df['Time stamp'], format='%b %d, %I:%M %p').dt.strftime('%b %d %H:%M:%S.%f') \u68c0\u67e5\u6570\u636e\u96c6\u7f3a\u5931\u7684\u503c # df.isnull().sum() \u68c0\u67e5\u6570\u636e\u96c6\u4e2d\u91cd\u590d\u51fa\u73b0\u7684\u503c\uff08\u5b9e\u9645\u4e0a\u6211\u4eec\u5e76\u4e0d\u60f3\u8981\uff09 # # \u8f93\u51fa\u91cd\u590d\u6570\u636e\u7684\u6570\u91cf df.duplicated().sum() # \u7528\u5e03\u5c14\u503c\u8bf4\u660e\u6570\u636e\u662f\u5426\u91cd\u590d(\u6700\u5f00\u59cb\u7684\u90a3\u4e00\u4e2a\u663e\u793aFalse) df.duplicated() # \u7b5b\u9009\u75c5\u5c55\u793a\u91cd\u590d\u6570\u636e df[df.duplicated()] # \u53bb\u6389\u91cd\u590d\u7684\u6570\u636e df.drop_duplicates() \u83b7\u53d6\u6570\u636e\u96c6\u7684\u6bcf\u4e00\u5217\u7684\u7edf\u8ba1\u4fe1\u606f # # round(2) \u4fdd\u7559\u5c0f\u6570\u70b9\u540e\u4e24\u4f4d df.describe().round(2) \u6dfb\u52a0\u4e00\u5217\u7279\u5f81(\u7279\u5f81\u5de5\u7a0b\u7684\u57fa\u7840) # # \u6dfb\u52a0\u4e00\u5217\u7279\u5f81\uff0c\u8fd9\u4e2a\u7279\u5f81\u662f\u6708\u4efd df[\"Month\"]=df[\"Time stamp\"].dt.month \u663e\u793a\u7279\u5f81\u5de5\u7a0b\u540e\u7684\u5e8f\u5217\u7684\u4fe1\u606f # df.info() \u4f7f\u7528\u4e24\u5217\u753b\u51fa\u6563\u70b9\u56fe(\u4f7f\u7528\u6563\u70b9\u56fe\u753b\u51fa\u65f6\u95f4\u5e8f\u5217) # plt.figure(figsize=(17,8)) # Time stamp\u662f\u4e00\u5217 # System power generated | (kW) \u662f\u4e00\u5217 sns.scatterplot(data=df,x=\"Time stamp\",y=\"System power generated | (kW)\",) plt.title(\"Effect the time by the system power\") plt.show() \u8bbe\u7f6e\u7d22\u5f15\u5e76\u4e14\u5728\u539f\u6765\u7684\u6570\u636e\u4e0a\u66f4\u6539 # # \u7d22\u5f15\u662f\u5217\u540d\u4e3aTime stamp\u7684\u5217 df.set_index(\"Time stamp\",inplace=True) \u5212\u5206\u9884\u6d4b\u7684\u503c\u548c\u7279\u5f81 # \u8fd9\u662f\u7531\u4e8e\u7279\u5f81\u548c\u76ee\u6807\u5728\u540c\u4e00\u4e2a\u6570\u636e\u5217\u8868\u91cc\u9762\uff0c\u6240\u4ee5\u9700\u8981\u5206\u5f00 # X \u662f\u7279\u5f81 X = df.drop(columns=\"System power generated | (kW)\") # y \u662f\u8981\u9884\u6d4b\u7684\u91cf y = df[\"System power generated | (kW)\"] # y = system power generated \u5212\u5206\u6570\u636e\u96c6 # # \u5212\u5206\u6570\u636e\u96c6 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42) # \u6253\u5370\u6570\u636e\u7684\u6570\u91cf\uff0c\u6ce8\u610fy\u5e94\u8be5\u662f\u5f62\u5982(2628\uff0c),\u800cX\u5e94\u8be5\u662f\u5f62\u5982(6132,5)\uff0c5\u662f\u7279\u5f81\u6570 # 5\u5f53\u4e2d \u4e0d\u5305\u542btimestamp\u8fd9\u4e00\u5217 print(\"X Train : \", X_train.shape) print(\"X Test : \", X_test.shape) print(\"Y Train : \", y_train.shape) print(\"Y Test : \", y_test.shape) \u5b9a\u4e49\u6a21\u578b\uff1a \u8fd9\u91cc\u4f7f\u7528tensorflow\u6765\u5b9e\u73b0 # LR = LinearRegression() DTR = DecisionTreeRegressor() RFR = RandomForestRegressor() KNR = KNeighborsRegressor() MLP = MLPRegressor() XGB = XGBRegressor() SVR = SVR() \u5bf9\u5df2\u7ecf\u5b9a\u4e49\u597d\u7684\u6a21\u578b\u5b9e\u884c\u9884\u6d4b\uff0c\u5e76\u4e14\u8f93\u51faMSE\u7684\u8bef\u5dee\u503c # # \u4e0d\u540c\u7684\u8bad\u7ec3\u597d\u7684\u6a21\u578b li = [LR,DTR,RFR,KNR,KNR,XGB,SVR] # \u8bef\u5dee\u5217\u8868 d = {} for i in li: # \u8fdb\u884c\u9884\u6d4b i.fit(X_train,y_train) #\u8ba1\u7b97\u8bef\u5dee ypred = i.predict(X_test) # \u6253\u5370\u51fa\u6765\u8bef\u5dee print(i,\":\",r2_score(y_test,ypred)*100) # \u4fdd\u5b58\u8bef\u5dee d.update({str(i):i.score(X_test,y_test)*100}) \u6253\u5370\u6bcf\u79cd\u9884\u6d4b\u6a21\u578b\u7684\u8bef\u5dee # plt.figure(figsize=(30, 6)) plt.title(\"Algorithm vs Accuracy\") plt.xlabel(\"Algorithm\") plt.ylabel(\"Accuracy\") plt.plot(d.keys(),d.values(),marker='o',color='red') plt.show() \u4f7f\u7528\u81ea\u5b9a\u4e49\u51fd\u6570\u6765\u8ba1\u7b97\u8bef\u5dee # # \u5f97\u5230\u9884\u6d4b\u5e8f\u5217 ypred_xgb=XGB.predict(X_test) # \u8ba1\u7b97\u8bef\u5dee import math from sklearn.metrics import mean_squared_error math.sqrt(mean_squared_error(y_test,ypred_xgb)) Tensorflow\u4fdd\u5b58\u6a21\u578b # import pickle pickle.dump(XGB,open('model.pkl','wb')) \u6311\u9009\u51fa\u4e00\u4e2a # # \u5bfc\u5165\u5bf9\u5e94\u7684\u5305 import numpy as np from tensorflow.keras.models import Sequential from tensorflow.keras.layers import LSTM from tensorflow.keras.layers import Dense from tensorflow.keras.layers import Flatten df.head() df1=df.reset_index()['System power generated | (kW)'] df1.shape","title":"Exploratory Data Analyses"},{"location":"Research/Multivariate_EDA/#multivariate-exploratory-data-analyseseda","text":"Exploratory Data Analyses (EDA) \u53eb\u505a\u6570\u636e\u63a2\u7d22\u6027\u5206\u6790\uff0c\u662f\u7528\u6765\u5728\u5bf9\u65f6\u95f4\u5e8f\u5217\u5904\u7406\u524d\u5148\u8fdb\u884c\u6570\u636e \u5355\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u662f\u6709\u591a\u4e2a\u53d8\u91cf\u7684\uff08\u9664\u65f6\u95f4\u5916\uff09\u7684\u5e8f\u5217\uff0c\u4e0b\u9762\u6211\u4eec\u8003\u8651\u5355\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u7684EDA\u95ee\u9898","title":"Multivariate Exploratory Data Analyses\uff08EDA)"},{"location":"Research/Multivariate_EDA/#_1","text":"import pandas as pd df=pd.read_csv('TexasTurbine.csv') # \u6253\u5370\u524d\u4e8c\u5341\u884c df.head(20)","title":"\u5bfc\u5165\u65f6\u95f4\u5e8f\u5217"},{"location":"Research/Multivariate_EDA/#_2","text":"import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt plt.style.use(\"Solarize_Light2\") #using style ggplot %matplotlib inline from mpl_toolkits.mplot3d import Axes3D import datetime as dt import plotly.graph_objects as go import plotly.express as px import datetime as dt","title":"\u5bfc\u5165\u5fc5\u8981\u7684\u5305"},{"location":"Research/Multivariate_EDA/#_3","text":"df.shape","title":"\u83b7\u53d6\u6570\u636e\u70b9\u7684\u6570\u91cf(\u6570\u636e\u63cf\u8ff0\u7684\u65f6\u5019\u4f7f\u7528)"},{"location":"Research/Multivariate_EDA/#_4","text":"df.info()","title":"\u83b7\u53d6\u6570\u636e\u7684\u7c7b\u578b"},{"location":"Research/Multivariate_EDA/#timestampstrftime","text":"df['Time stamp'] = pd.to_datetime(df['Time stamp'], format='%b %d, %I:%M %p').dt.strftime('%b %d %H:%M:%S.%f')","title":"\u628aTimestamp\u8f6c\u5316\u4e3astrftime\u7c7b\u578b"},{"location":"Research/Multivariate_EDA/#_5","text":"df.isnull().sum()","title":"\u68c0\u67e5\u6570\u636e\u96c6\u7f3a\u5931\u7684\u503c"},{"location":"Research/Multivariate_EDA/#_6","text":"# \u8f93\u51fa\u91cd\u590d\u6570\u636e\u7684\u6570\u91cf df.duplicated().sum() # \u7528\u5e03\u5c14\u503c\u8bf4\u660e\u6570\u636e\u662f\u5426\u91cd\u590d(\u6700\u5f00\u59cb\u7684\u90a3\u4e00\u4e2a\u663e\u793aFalse) df.duplicated() # \u7b5b\u9009\u75c5\u5c55\u793a\u91cd\u590d\u6570\u636e df[df.duplicated()] # \u53bb\u6389\u91cd\u590d\u7684\u6570\u636e df.drop_duplicates()","title":"\u68c0\u67e5\u6570\u636e\u96c6\u4e2d\u91cd\u590d\u51fa\u73b0\u7684\u503c\uff08\u5b9e\u9645\u4e0a\u6211\u4eec\u5e76\u4e0d\u60f3\u8981\uff09"},{"location":"Research/Multivariate_EDA/#_7","text":"# round(2) \u4fdd\u7559\u5c0f\u6570\u70b9\u540e\u4e24\u4f4d df.describe().round(2)","title":"\u83b7\u53d6\u6570\u636e\u96c6\u7684\u6bcf\u4e00\u5217\u7684\u7edf\u8ba1\u4fe1\u606f"},{"location":"Research/Multivariate_EDA/#_8","text":"# \u6dfb\u52a0\u4e00\u5217\u7279\u5f81\uff0c\u8fd9\u4e2a\u7279\u5f81\u662f\u6708\u4efd df[\"Month\"]=df[\"Time stamp\"].dt.month","title":"\u6dfb\u52a0\u4e00\u5217\u7279\u5f81(\u7279\u5f81\u5de5\u7a0b\u7684\u57fa\u7840)"},{"location":"Research/Multivariate_EDA/#_9","text":"df.info()","title":"\u663e\u793a\u7279\u5f81\u5de5\u7a0b\u540e\u7684\u5e8f\u5217\u7684\u4fe1\u606f"},{"location":"Research/Multivariate_EDA/#_10","text":"plt.figure(figsize=(17,8)) # Time stamp\u662f\u4e00\u5217 # System power generated | (kW) \u662f\u4e00\u5217 sns.scatterplot(data=df,x=\"Time stamp\",y=\"System power generated | (kW)\",) plt.title(\"Effect the time by the system power\") plt.show()","title":"\u4f7f\u7528\u4e24\u5217\u753b\u51fa\u6563\u70b9\u56fe(\u4f7f\u7528\u6563\u70b9\u56fe\u753b\u51fa\u65f6\u95f4\u5e8f\u5217)"},{"location":"Research/Multivariate_EDA/#_11","text":"# \u7d22\u5f15\u662f\u5217\u540d\u4e3aTime stamp\u7684\u5217 df.set_index(\"Time stamp\",inplace=True)","title":"\u8bbe\u7f6e\u7d22\u5f15\u5e76\u4e14\u5728\u539f\u6765\u7684\u6570\u636e\u4e0a\u66f4\u6539"},{"location":"Research/Multivariate_EDA/#_12","text":"\u8fd9\u662f\u7531\u4e8e\u7279\u5f81\u548c\u76ee\u6807\u5728\u540c\u4e00\u4e2a\u6570\u636e\u5217\u8868\u91cc\u9762\uff0c\u6240\u4ee5\u9700\u8981\u5206\u5f00 # X \u662f\u7279\u5f81 X = df.drop(columns=\"System power generated | (kW)\") # y \u662f\u8981\u9884\u6d4b\u7684\u91cf y = df[\"System power generated | (kW)\"] # y = system power generated","title":"\u5212\u5206\u9884\u6d4b\u7684\u503c\u548c\u7279\u5f81"},{"location":"Research/Multivariate_EDA/#_13","text":"# \u5212\u5206\u6570\u636e\u96c6 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42) # \u6253\u5370\u6570\u636e\u7684\u6570\u91cf\uff0c\u6ce8\u610fy\u5e94\u8be5\u662f\u5f62\u5982(2628\uff0c),\u800cX\u5e94\u8be5\u662f\u5f62\u5982(6132,5)\uff0c5\u662f\u7279\u5f81\u6570 # 5\u5f53\u4e2d \u4e0d\u5305\u542btimestamp\u8fd9\u4e00\u5217 print(\"X Train : \", X_train.shape) print(\"X Test : \", X_test.shape) print(\"Y Train : \", y_train.shape) print(\"Y Test : \", y_test.shape)","title":"\u5212\u5206\u6570\u636e\u96c6"},{"location":"Research/Multivariate_EDA/#tensorflow","text":"LR = LinearRegression() DTR = DecisionTreeRegressor() RFR = RandomForestRegressor() KNR = KNeighborsRegressor() MLP = MLPRegressor() XGB = XGBRegressor() SVR = SVR()","title":"\u5b9a\u4e49\u6a21\u578b\uff1a \u8fd9\u91cc\u4f7f\u7528tensorflow\u6765\u5b9e\u73b0"},{"location":"Research/Multivariate_EDA/#mse","text":"# \u4e0d\u540c\u7684\u8bad\u7ec3\u597d\u7684\u6a21\u578b li = [LR,DTR,RFR,KNR,KNR,XGB,SVR] # \u8bef\u5dee\u5217\u8868 d = {} for i in li: # \u8fdb\u884c\u9884\u6d4b i.fit(X_train,y_train) #\u8ba1\u7b97\u8bef\u5dee ypred = i.predict(X_test) # \u6253\u5370\u51fa\u6765\u8bef\u5dee print(i,\":\",r2_score(y_test,ypred)*100) # \u4fdd\u5b58\u8bef\u5dee d.update({str(i):i.score(X_test,y_test)*100})","title":"\u5bf9\u5df2\u7ecf\u5b9a\u4e49\u597d\u7684\u6a21\u578b\u5b9e\u884c\u9884\u6d4b\uff0c\u5e76\u4e14\u8f93\u51faMSE\u7684\u8bef\u5dee\u503c"},{"location":"Research/Multivariate_EDA/#_14","text":"plt.figure(figsize=(30, 6)) plt.title(\"Algorithm vs Accuracy\") plt.xlabel(\"Algorithm\") plt.ylabel(\"Accuracy\") plt.plot(d.keys(),d.values(),marker='o',color='red') plt.show()","title":"\u6253\u5370\u6bcf\u79cd\u9884\u6d4b\u6a21\u578b\u7684\u8bef\u5dee"},{"location":"Research/Multivariate_EDA/#_15","text":"# \u5f97\u5230\u9884\u6d4b\u5e8f\u5217 ypred_xgb=XGB.predict(X_test) # \u8ba1\u7b97\u8bef\u5dee import math from sklearn.metrics import mean_squared_error math.sqrt(mean_squared_error(y_test,ypred_xgb))","title":"\u4f7f\u7528\u81ea\u5b9a\u4e49\u51fd\u6570\u6765\u8ba1\u7b97\u8bef\u5dee"},{"location":"Research/Multivariate_EDA/#tensorflow_1","text":"import pickle pickle.dump(XGB,open('model.pkl','wb'))","title":"Tensorflow\u4fdd\u5b58\u6a21\u578b"},{"location":"Research/Multivariate_EDA/#_16","text":"# \u5bfc\u5165\u5bf9\u5e94\u7684\u5305 import numpy as np from tensorflow.keras.models import Sequential from tensorflow.keras.layers import LSTM from tensorflow.keras.layers import Dense from tensorflow.keras.layers import Flatten df.head() df1=df.reset_index()['System power generated | (kW)'] df1.shape","title":"\u6311\u9009\u51fa\u4e00\u4e2a"},{"location":"Research/Paper%20Title%20Deterministic%20and%20Probabilistic%20Wind%20Power%20Forecasts%20by%20Considering%20Various%20Atmospheric%20Models%20and%20Feature/","text":"Paper 1 title: # Deterministic and Probabilistic Wind Power Forecasts by Considering Various Atmospheric Models and Feature Engineering Approaches Executive summary: The authors use three kind of numerical weather prediction (wind speed) rather than the single wind speed from the anemometer to serve as the feature engineering. The kernel of this paper is to use the specific data source to construct the feature and hence. What is useful for me is that we can consider the feature engineering in our work and cite this to alleviate the boring description . For reference, I think the following reference are of value: A New Fuzzy-Based Combined Prediction Interval for Wind Power Forecasting Technical description: PI coverage probability : show the percentage of the probability targets which will be covered by the upper and lower bound. $$ \\mathrm{PICP}=\\frac{1}{N} \\sum_{t=1}^N c_t $$ where \\(N\\) is the number of samples and \\(c_t\\) is a Boolean value that is evaluated as follows: $$ c_t= \\begin{cases}1, & y_t \\in\\left[L_t, U_t\\right] \\ 0, & y_t \\notin\\left[L_t, U_t\\right]\\end{cases} $$ where \\(y_t\\) is the forecast target and \\(U_t\\) and \\(L_t\\) are upper and lower bounds of the interval, respectively. PI normalized average width (PINAW): limit the high extra growth of the interval $$ \\text { PINAW }=\\frac{1}{N R} \\sum_{t=1}^N\\left(U_t-L_t\\right) $$ where \\(R\\) is the range of the underlying targets used for normalizing PIs. The LUBE method could be regarded as a constrained nonlinear optimization problem with conflicting objective as follows: Objectives : Maximize: \\(\\operatorname{PICP}(w)\\) Minimize: PINAW \\((w)\\) Constraints : $$ 0 \\leq \\operatorname{PICP}(w) \\leq 100 \\% $$ \u200b PINAW \\((w) \\succ 0\\) This is resolved by the following method: $$ \\begin{aligned} & F(X)=\\min {X \\in \\Omega}\\left{\\max {i=1, \\ldots, n}\\left|\\mu_{\\text {ref }, i}-\\mu_{f, i}(X)\\right|\\right} \\ &=\\min {X \\in \\Omega}\\left{\\operatorname { m a x } \\left(\\left|\\mu {\\text {ref }, \\mathrm{PICP}}-\\mu_{\\mathrm{PICP}}(X)\\right|\\right.\\right. \\ &\\left.\\left.\\left|\\mu_{\\text {ref }, \\mathrm{PINAW}}-\\mu_{\\mathrm{PNAW}}(X)\\right|\\right)\\right} \\end{aligned} $$ where \\(n\\) is the number of objectives (here \\(n=2\\) ), \\(\\mu_{f, i}\\) is the membership function value of the \\(i\\) th objective, \\(\\Omega\\) is the problem search space; \\(X\\) is the control vector including the \\(\\mathrm{NN}\\) weighting factors, and \\(\\mu_{\\mathrm{ref}, i}\\) is the reference membership value for \\(i\\) -th objective. Paper 2 Title: # Privacy-preserving Spatiotemporal Scenario Generation of Renewable Energies: A Federated Deep Generative Learning Approach Executive summary: The authors want to use the federated learning with a central server to generate the scenarios for the wind power. The authors use the federated learning and the least square generative adversarial networks (LSGANs) for renewable scenario generation. What I think is useful for me is the concept of scenario generation and the application of federated learning. There are some references that I think is interesting: Technical description: Generative adversarial networks : GAN contains discriminator and generator, the generator is used to generated samples and the discriminator is used to judge the input data whether historical data or the generated data as much as possible. then the output of the discriminator network is $$ \\left{\\begin{array}{l} p_{\\text {real }}=D(\\boldsymbol{x}) \\ p_{\\text {fake }}=D(G(\\boldsymbol{z})) \\end{array}\\right. $$ and the loss function of generated and discriminator are $$ \\begin{gathered} L_G=\\mathbb{E} {\\boldsymbol{z} \\sim P_Z}[\\log (1-D(G(\\boldsymbol{z})))] \\ L_D=-\\mathbb{E} {\\boldsymbol{x} \\sim P_d}[\\log D(\\boldsymbol{x})]-\\mathbb{E} {\\boldsymbol{z} \\sim P_d}[\\log (1-D(G(\\boldsymbol{z})))] \\end{gathered} $$ where \\(P_Z\\) is a known distribution that is easy to sample. Then the mini-max game model with value function \\(V_{\\mathrm{GANs}}(G, D)\\) is given by $$ \\begin{aligned} \\min _G \\max _D V {\\mathrm{GANs}}(G, D)= & \\mathbb{E} {\\boldsymbol{x} \\sim P_d}[\\log D(\\boldsymbol{x})] \\ & +\\mathbb{E} {\\boldsymbol{z} \\sim P_z}[\\log (1-D(G(\\boldsymbol{z})))] \\end{aligned} $$ Federated Learning: Suppose there are \\(N\\) clients, i.e. participating edge devices \\(\\left\\{\\mathcal{C}_1, \\mathcal{C}_2, \\ldots, \\mathcal{C}_N\\right\\}\\) based on their own dataset \\(\\left\\{\\mathcal{D}_1, \\mathcal{D}_2, \\ldots, \\mathcal{D}_N\\right\\}\\) traditional way: put all data together and train a big model Federated learning coordinates clients to train a global model \\(\\mathcal{M}_{\\mathrm{FED}}\\) deployed on a central server, not collecting all data. \\(\\delta\\) -accuracy loss : assuming that \\(\\mathcal{V}_{\\text {SUM }}\\) and \\(\\mathcal{V}_{\\mathrm{FED}}\\) are the performance metrics of the centralized model \\(\\mathcal{M}_{\\text {SUM }}\\) and federated model \\(\\mathcal{M}_{\\text {FED }}\\) , then \\(\\left|\\mathcal{V}_{\\mathrm{SUM}}-\\mathcal{V}_{\\mathrm{FED}}\\right|<\\delta\\) Global LSGANs Model : Traditionally, the generator is fixed, the optimal discriminator is as follows: $$ D_{G, \\mathrm{GANs}}^*(\\boldsymbol{x})=\\frac{P_d(\\boldsymbol{x})}{P_d(\\boldsymbol{x})+P_g(\\boldsymbol{x})} $$ New: Substitute the above equation into $$ \\begin{aligned} \\min G \\max _D V {\\mathrm{GANs}}(G, D)= & \\mathbb{E} {\\boldsymbol{x} \\sim P_d}[\\log D(\\boldsymbol{x})] \\ & +\\mathbb{E} {\\boldsymbol{z} \\sim P_z}[\\log (1-D(G(\\boldsymbol{z})))] \\end{aligned} $$ then we could get $$ C_{\\mathrm{GANs}}(G)=V\\left(D_{D, \\mathrm{GANs}}^ , G\\right)=2 \\mathrm{JSD}\\left(P_d | P_g\\right)-\\log (4) $$ There are some drawbacks of the GAN, then least square-GAN are proposed. use \\(a-b\\) encoding and the least squares loss function, then the objective function of LSGAN is $$ \\begin{gathered} \\min D V {\\mathrm{LSGANs}}(D)=\\frac{1}{2} \\mathbb{E} {\\boldsymbol{x} \\sim P_d}\\left[(D(\\boldsymbol{x})-b)^2\\right]+ \\ \\frac{1}{2} \\mathbb{E} {\\boldsymbol{z} \\sim P_Z}\\left[(D(G(\\boldsymbol{z}))-a)^2\\right] \\ \\min G V {\\mathrm{LSGANs}}(G)=\\frac{1}{2} \\mathbb{E} {\\boldsymbol{z} \\sim P_Z}\\left[(D(G(\\boldsymbol{z}))-c)^2\\right] \\end{gathered} $$ The for the generator \\(G\\) , the optimal discriminator \\(D\\) is $$ D {G, \\text { LSGANs }}^ (\\boldsymbol{x})=\\frac{b P_d(\\boldsymbol{x})+a P_g(\\boldsymbol{x})}{P_d(\\boldsymbol{x})+P_g(\\boldsymbol{x})} $$ If we choose \\(b-c=1\\) and \\(b-a=2\\) , then we could get $$ 2 C_{\\mathrm{LSGANs}}(G)=\\chi_{\\text {Pearson }}^2\\left(P_d+P_g | 2 P_g\\right) $$ where \\(\\chi_{\\text {Pearson }}^2\\) is the Pearson \\(\\chi^2\\) divergence. If \\(b-c=1\\) and \\(b-a=2\\) are satisfied, (8) is equivalent to minimize the Pearson \\(\\chi^2\\) divergence. network configuration: activation function, ReLU and LeakyReLU activation functions Then we consider the FederatedAveraging (FedAvg) algorithm. This algorithm is proposed in this paper: Communication-Efficient Learning of Deep Networks from Decentralized Data The major difference between federated optimization and distribution optimization. Non-IID: any particular user's local dataset will not be representative of the population distribution Unbalanced: Some users will make much heavier use of the service Massively distributed: Limited communication, Mobile devices are frequently offline or on slow or expensive connection. \\[ \\min _{w \\in \\mathbb{R}^d} f(w) \\quad \\text { where } \\quad f(w) \\stackrel{\\text { def }}{=} \\frac{1}{n} \\sum_{i=1}^n f_i(w) \\] For a machine learning problem, we typically take $$ f_i(w)= \\ell\\left(x_i, y_i ; w\\right) $$ We assume that there are \\(K\\) clients and \\(\\mathcal{P}_k\\) the set of indexes of data points on client \\(k\\) , with \\(n_k = \\lvert \\mathcal{P}_k \\rvert\\) , then $$ f(w)=\\sum_{k=1}^K \\frac{n_k}{n} F_k(w) \\quad \\text { where } \\quad F_k(w)=\\frac{1}{n_k} \\sum_{i \\in \\mathcal{P} k} f_i(w) \\text {. } $$ three key parameters: \\(C\\) , the fraction of clients that perform computation on each round; \\(E\\) , then number of training passes each client makes over its local dataset on each round; and \\(B\\) , the local minibatch size used for the client updates. We write \\(B=\\infty\\) to indicate that the full data are used. $$ \\begin{aligned} & \\hline \\text { Algorithm } 1 \\text { FederatedAveraging. The } K \\text { clients are } \\ & \\text { indexed by } k ; B \\text { is the local minibatch size, } E \\text { is the number } \\ & \\text { of local epochs, and } \\eta \\text { is the learning rate. } \\ & \\hline \\text { Server executes: } \\ & \\text { initialize } w_0 \\ & \\text { for each round } t=1,2, \\ldots \\text { do } \\ & \\quad m \\leftarrow \\max (C \\cdot K, 1) \\ & \\quad S_t \\leftarrow(\\operatorname{random} \\text { set of } m \\text { clients) } \\ & \\quad \\text { for each client } k \\in S_t \\text { in parallel do } \\ & \\quad w {t+1}^k \\leftarrow \\text { ClientUpdate }\\left(k, w_t\\right) \\ & \\quad w_{t+1} \\leftarrow \\sum_{k=1}^K \\frac{n_k}{n} w_{t+1}^k \\ & \\text { ClientUpdate }(k, w): / / \\text { Run on client } k \\ & \\mathcal{B} \\leftarrow\\left(\\text { split } \\mathcal{P}_k \\text { into batches of size } B\\right) \\ & \\text { for each local epoch } i \\text { from } 1 \\text { to } E \\text { do } \\ & \\text { for batch } b \\in \\mathcal{B} \\text { do } \\ & \\quad w \\leftarrow w-\\eta \\nabla \\ell(w ; b) \\ & \\text { return } w \\text { to server } \\end{aligned} $$ Then the algorithm could be (there is no contribution, except the GANs optimization part.) \\(\\overline{\\text { Algorithm } 1 \\text { Federated Least Squares Generative Adversarial }}\\) Networks (Fed-LSGAN) Require: Learning rate \\(\\alpha\\) ; Adam hyperparameters \\(\\beta_1, \\beta_2\\) ; Local minibatch size \\(m\\) ; Global epoch \\(W\\) ; Synchronization interval \\(K\\) , Client ratio \\(E\\) . Require: A global LSGANs model with parameters \\(\\left(\\theta_g^{\\mathcal{S}}, \\theta_d^{\\mathcal{S}}\\right)\\) for Discriminator and Generator on central server \\(\\mathcal{S}\\) ; local LSGANs models with parameters \\(\\left\\{\\theta_g, \\theta_d\\right\\}_{i=1}^{N_r}\\) on \\(N_r\\) clients \\(\\{\\mathcal{C}\\}_{i=1}^{N_r}\\) . 8: Update generator \\(\\theta_g^e\\) of client \\(e\\) - Sample batch noise samples \\(\\{z\\}_{i=1}^m \\sim P_Z\\) . - Update parameters for generator network $$ \\left{\\begin{array}{l} g_{\\theta_g} \\leftarrow \\nabla_{\\theta_g} \\frac{1}{m} \\sum_{i=1}^m \\frac{1}{2}\\left(D\\left(G\\left(z^{(i)}\\right)\\right)-1\\right)^2 \\ \\theta_g \\leftarrow \\theta_g-\\alpha \\cdot \\operatorname{Adam}\\left(\\theta_g, g_{\\theta_g}, \\beta_1, \\beta_2\\right) \\end{array}\\right. $$ 11: end for 12: \\(\\quad\\) if \\(w \\bmod K=0\\) then 13: All selected clients send parameters to server, and the server aggregate \\(\\theta^{\\mathcal{S}}\\) by averaging $$ \\theta_g^S \\leftarrow \\frac{1}{N_e} \\sum_{e=1}^{N_e} \\theta_g^e, \\quad \\theta_d^{\\mathcal{S}} \\leftarrow \\frac{1}{N_e} \\sum_{e=1}^{N_e} \\theta_d^e $$ 14: The server send back parameters and clients update local parameters $$ \\left{\\theta_g, \\theta_d\\right}_{i=1}^{N_r} \\leftarrow\\left(\\theta_g^{\\mathcal{S}}, \\theta_d^{\\mathcal{S}}\\right) $$ 15: end if Metric: Performance metric Correlation Analysis: $$ R(\\tau)=\\frac{\\mathbb{E}\\left[\\left(S_t-\\mu\\right)\\left(S_{t+\\tau}-\\mu\\right)\\right]}{\\sigma^2} $$ where \\(S\\) is a random time series; \\(\\mu\\) and \\(\\sigma\\) denote the mean and variance of \\(S\\) , respectively; and \\(\\tau\\) is the time lag. We use the continuous ranked probability score ( CRPS ) which measures the dissimilarity of the cumulative distributions between generated scenarios and historical observations. The score at lead time \\(\\ell\\) is defined as $$ \\operatorname{CRPS} l=\\frac{1}{M} \\sum {t=1}^M \\int_0^1\\left(\\widehat{F} {t+l \\mid t}(\\xi)-\\mathbf{1}\\left(\\xi \\geq \\xi {t+l}\\right)\\right)^2 d \\xi $$ where \\(M\\) is the total number of scenarios, \\(\\widehat{F}_{t+l \\mid t}(\\xi)\\) denotes the cumulative distribution function of normalized scenario, and \\(\\mathbf{1}\\left(\\xi \\geq \\xi_{t+l}\\right)\\) is the indicator function for comparing scenarios and observation. Fr\u00e9chet inception Distance (FID) $$ \\operatorname{FID}\\left(P_d, P_g\\right)=\\left|\\mu_d-\\mu_g\\right|+\\operatorname{Tr}\\left(\\Sigma_d+\\Sigma_g-2\\left(\\Sigma_d \\Sigma_g\\right)^{\\frac{1}{2}}\\right) $$ where \\(\\mu_d\\) and \\(\\mu_g\\) represent the empirical mean; \\(\\Sigma_d\\) and \\(\\Sigma_g\\) are empirical covariance. Kernel Maximum Mean Discrepancy (MMD): measures the difference between \\(P_d\\) and \\(P_g\\) for some fixed kernel function \\(k\\) , which is defined as $$ \\operatorname{MMD}^2\\left(P_d, P_g\\right)=\\underset{\\substack{x, x^{\\prime} \\sim P_d \\ y, y^{\\prime} \\sim P_g}}{ }\\left[k\\left(x, x^{\\prime}\\right)-2 k(x, y)+k\\left(y, y^{\\prime}\\right)\\right] $$ The 1-Nearest Neighbor classifier Energy Score (ES) $$ \\mathrm{ES}=\\frac{1}{M} \\sum_{i=1}^M\\left|\\varsigma-\\xi_i\\right|-\\frac{1}{2 M^2} \\sum_{i=1}^M \\sum_{j=1}^M\\left|\\xi_i-\\xi_j\\right| $$ \\(\\varsigma\\) is the real renewable power output, \\(\\xi_i\\) is the \\(i\\) -th generated time series scenario and \\(M\\) denotes the number of scenarios. Pearson correlation coefficient \\(\\rho\\) of two time series \\(S_i\\) and \\(S_j\\) is $$ \\rho\\left(S_i, S_j\\right)=\\frac{\\sum_{i=1}^n\\left(S_i-\\bar{S} i\\right)\\left(S_j-\\bar{S}_j\\right)}{\\sqrt{\\sum {i=1}^n\\left(S_i-\\bar{S} i\\right)^2} \\sqrt{\\sum {i=1}^n\\left(S_j-\\bar{S}_j\\right)^2}} $$ #","title":"Paper Title Deterministic and Probabilistic Wind Power Forecasts by Considering Various Atmospheric Models and Feature"},{"location":"Research/Paper%20Title%20Deterministic%20and%20Probabilistic%20Wind%20Power%20Forecasts%20by%20Considering%20Various%20Atmospheric%20Models%20and%20Feature/#paper-1-title","text":"Deterministic and Probabilistic Wind Power Forecasts by Considering Various Atmospheric Models and Feature Engineering Approaches Executive summary: The authors use three kind of numerical weather prediction (wind speed) rather than the single wind speed from the anemometer to serve as the feature engineering. The kernel of this paper is to use the specific data source to construct the feature and hence. What is useful for me is that we can consider the feature engineering in our work and cite this to alleviate the boring description . For reference, I think the following reference are of value: A New Fuzzy-Based Combined Prediction Interval for Wind Power Forecasting Technical description: PI coverage probability : show the percentage of the probability targets which will be covered by the upper and lower bound. $$ \\mathrm{PICP}=\\frac{1}{N} \\sum_{t=1}^N c_t $$ where \\(N\\) is the number of samples and \\(c_t\\) is a Boolean value that is evaluated as follows: $$ c_t= \\begin{cases}1, & y_t \\in\\left[L_t, U_t\\right] \\ 0, & y_t \\notin\\left[L_t, U_t\\right]\\end{cases} $$ where \\(y_t\\) is the forecast target and \\(U_t\\) and \\(L_t\\) are upper and lower bounds of the interval, respectively. PI normalized average width (PINAW): limit the high extra growth of the interval $$ \\text { PINAW }=\\frac{1}{N R} \\sum_{t=1}^N\\left(U_t-L_t\\right) $$ where \\(R\\) is the range of the underlying targets used for normalizing PIs. The LUBE method could be regarded as a constrained nonlinear optimization problem with conflicting objective as follows: Objectives : Maximize: \\(\\operatorname{PICP}(w)\\) Minimize: PINAW \\((w)\\) Constraints : $$ 0 \\leq \\operatorname{PICP}(w) \\leq 100 \\% $$ \u200b PINAW \\((w) \\succ 0\\) This is resolved by the following method: $$ \\begin{aligned} & F(X)=\\min {X \\in \\Omega}\\left{\\max {i=1, \\ldots, n}\\left|\\mu_{\\text {ref }, i}-\\mu_{f, i}(X)\\right|\\right} \\ &=\\min {X \\in \\Omega}\\left{\\operatorname { m a x } \\left(\\left|\\mu {\\text {ref }, \\mathrm{PICP}}-\\mu_{\\mathrm{PICP}}(X)\\right|\\right.\\right. \\ &\\left.\\left.\\left|\\mu_{\\text {ref }, \\mathrm{PINAW}}-\\mu_{\\mathrm{PNAW}}(X)\\right|\\right)\\right} \\end{aligned} $$ where \\(n\\) is the number of objectives (here \\(n=2\\) ), \\(\\mu_{f, i}\\) is the membership function value of the \\(i\\) th objective, \\(\\Omega\\) is the problem search space; \\(X\\) is the control vector including the \\(\\mathrm{NN}\\) weighting factors, and \\(\\mu_{\\mathrm{ref}, i}\\) is the reference membership value for \\(i\\) -th objective.","title":"Paper 1 title:"},{"location":"Research/Paper%20Title%20Deterministic%20and%20Probabilistic%20Wind%20Power%20Forecasts%20by%20Considering%20Various%20Atmospheric%20Models%20and%20Feature/#paper-2-title","text":"Privacy-preserving Spatiotemporal Scenario Generation of Renewable Energies: A Federated Deep Generative Learning Approach Executive summary: The authors want to use the federated learning with a central server to generate the scenarios for the wind power. The authors use the federated learning and the least square generative adversarial networks (LSGANs) for renewable scenario generation. What I think is useful for me is the concept of scenario generation and the application of federated learning. There are some references that I think is interesting: Technical description: Generative adversarial networks : GAN contains discriminator and generator, the generator is used to generated samples and the discriminator is used to judge the input data whether historical data or the generated data as much as possible. then the output of the discriminator network is $$ \\left{\\begin{array}{l} p_{\\text {real }}=D(\\boldsymbol{x}) \\ p_{\\text {fake }}=D(G(\\boldsymbol{z})) \\end{array}\\right. $$ and the loss function of generated and discriminator are $$ \\begin{gathered} L_G=\\mathbb{E} {\\boldsymbol{z} \\sim P_Z}[\\log (1-D(G(\\boldsymbol{z})))] \\ L_D=-\\mathbb{E} {\\boldsymbol{x} \\sim P_d}[\\log D(\\boldsymbol{x})]-\\mathbb{E} {\\boldsymbol{z} \\sim P_d}[\\log (1-D(G(\\boldsymbol{z})))] \\end{gathered} $$ where \\(P_Z\\) is a known distribution that is easy to sample. Then the mini-max game model with value function \\(V_{\\mathrm{GANs}}(G, D)\\) is given by $$ \\begin{aligned} \\min _G \\max _D V {\\mathrm{GANs}}(G, D)= & \\mathbb{E} {\\boldsymbol{x} \\sim P_d}[\\log D(\\boldsymbol{x})] \\ & +\\mathbb{E} {\\boldsymbol{z} \\sim P_z}[\\log (1-D(G(\\boldsymbol{z})))] \\end{aligned} $$ Federated Learning: Suppose there are \\(N\\) clients, i.e. participating edge devices \\(\\left\\{\\mathcal{C}_1, \\mathcal{C}_2, \\ldots, \\mathcal{C}_N\\right\\}\\) based on their own dataset \\(\\left\\{\\mathcal{D}_1, \\mathcal{D}_2, \\ldots, \\mathcal{D}_N\\right\\}\\) traditional way: put all data together and train a big model Federated learning coordinates clients to train a global model \\(\\mathcal{M}_{\\mathrm{FED}}\\) deployed on a central server, not collecting all data. \\(\\delta\\) -accuracy loss : assuming that \\(\\mathcal{V}_{\\text {SUM }}\\) and \\(\\mathcal{V}_{\\mathrm{FED}}\\) are the performance metrics of the centralized model \\(\\mathcal{M}_{\\text {SUM }}\\) and federated model \\(\\mathcal{M}_{\\text {FED }}\\) , then \\(\\left|\\mathcal{V}_{\\mathrm{SUM}}-\\mathcal{V}_{\\mathrm{FED}}\\right|<\\delta\\) Global LSGANs Model : Traditionally, the generator is fixed, the optimal discriminator is as follows: $$ D_{G, \\mathrm{GANs}}^*(\\boldsymbol{x})=\\frac{P_d(\\boldsymbol{x})}{P_d(\\boldsymbol{x})+P_g(\\boldsymbol{x})} $$ New: Substitute the above equation into $$ \\begin{aligned} \\min G \\max _D V {\\mathrm{GANs}}(G, D)= & \\mathbb{E} {\\boldsymbol{x} \\sim P_d}[\\log D(\\boldsymbol{x})] \\ & +\\mathbb{E} {\\boldsymbol{z} \\sim P_z}[\\log (1-D(G(\\boldsymbol{z})))] \\end{aligned} $$ then we could get $$ C_{\\mathrm{GANs}}(G)=V\\left(D_{D, \\mathrm{GANs}}^ , G\\right)=2 \\mathrm{JSD}\\left(P_d | P_g\\right)-\\log (4) $$ There are some drawbacks of the GAN, then least square-GAN are proposed. use \\(a-b\\) encoding and the least squares loss function, then the objective function of LSGAN is $$ \\begin{gathered} \\min D V {\\mathrm{LSGANs}}(D)=\\frac{1}{2} \\mathbb{E} {\\boldsymbol{x} \\sim P_d}\\left[(D(\\boldsymbol{x})-b)^2\\right]+ \\ \\frac{1}{2} \\mathbb{E} {\\boldsymbol{z} \\sim P_Z}\\left[(D(G(\\boldsymbol{z}))-a)^2\\right] \\ \\min G V {\\mathrm{LSGANs}}(G)=\\frac{1}{2} \\mathbb{E} {\\boldsymbol{z} \\sim P_Z}\\left[(D(G(\\boldsymbol{z}))-c)^2\\right] \\end{gathered} $$ The for the generator \\(G\\) , the optimal discriminator \\(D\\) is $$ D {G, \\text { LSGANs }}^ (\\boldsymbol{x})=\\frac{b P_d(\\boldsymbol{x})+a P_g(\\boldsymbol{x})}{P_d(\\boldsymbol{x})+P_g(\\boldsymbol{x})} $$ If we choose \\(b-c=1\\) and \\(b-a=2\\) , then we could get $$ 2 C_{\\mathrm{LSGANs}}(G)=\\chi_{\\text {Pearson }}^2\\left(P_d+P_g | 2 P_g\\right) $$ where \\(\\chi_{\\text {Pearson }}^2\\) is the Pearson \\(\\chi^2\\) divergence. If \\(b-c=1\\) and \\(b-a=2\\) are satisfied, (8) is equivalent to minimize the Pearson \\(\\chi^2\\) divergence. network configuration: activation function, ReLU and LeakyReLU activation functions Then we consider the FederatedAveraging (FedAvg) algorithm. This algorithm is proposed in this paper: Communication-Efficient Learning of Deep Networks from Decentralized Data The major difference between federated optimization and distribution optimization. Non-IID: any particular user's local dataset will not be representative of the population distribution Unbalanced: Some users will make much heavier use of the service Massively distributed: Limited communication, Mobile devices are frequently offline or on slow or expensive connection. \\[ \\min _{w \\in \\mathbb{R}^d} f(w) \\quad \\text { where } \\quad f(w) \\stackrel{\\text { def }}{=} \\frac{1}{n} \\sum_{i=1}^n f_i(w) \\] For a machine learning problem, we typically take $$ f_i(w)= \\ell\\left(x_i, y_i ; w\\right) $$ We assume that there are \\(K\\) clients and \\(\\mathcal{P}_k\\) the set of indexes of data points on client \\(k\\) , with \\(n_k = \\lvert \\mathcal{P}_k \\rvert\\) , then $$ f(w)=\\sum_{k=1}^K \\frac{n_k}{n} F_k(w) \\quad \\text { where } \\quad F_k(w)=\\frac{1}{n_k} \\sum_{i \\in \\mathcal{P} k} f_i(w) \\text {. } $$ three key parameters: \\(C\\) , the fraction of clients that perform computation on each round; \\(E\\) , then number of training passes each client makes over its local dataset on each round; and \\(B\\) , the local minibatch size used for the client updates. We write \\(B=\\infty\\) to indicate that the full data are used. $$ \\begin{aligned} & \\hline \\text { Algorithm } 1 \\text { FederatedAveraging. The } K \\text { clients are } \\ & \\text { indexed by } k ; B \\text { is the local minibatch size, } E \\text { is the number } \\ & \\text { of local epochs, and } \\eta \\text { is the learning rate. } \\ & \\hline \\text { Server executes: } \\ & \\text { initialize } w_0 \\ & \\text { for each round } t=1,2, \\ldots \\text { do } \\ & \\quad m \\leftarrow \\max (C \\cdot K, 1) \\ & \\quad S_t \\leftarrow(\\operatorname{random} \\text { set of } m \\text { clients) } \\ & \\quad \\text { for each client } k \\in S_t \\text { in parallel do } \\ & \\quad w {t+1}^k \\leftarrow \\text { ClientUpdate }\\left(k, w_t\\right) \\ & \\quad w_{t+1} \\leftarrow \\sum_{k=1}^K \\frac{n_k}{n} w_{t+1}^k \\ & \\text { ClientUpdate }(k, w): / / \\text { Run on client } k \\ & \\mathcal{B} \\leftarrow\\left(\\text { split } \\mathcal{P}_k \\text { into batches of size } B\\right) \\ & \\text { for each local epoch } i \\text { from } 1 \\text { to } E \\text { do } \\ & \\text { for batch } b \\in \\mathcal{B} \\text { do } \\ & \\quad w \\leftarrow w-\\eta \\nabla \\ell(w ; b) \\ & \\text { return } w \\text { to server } \\end{aligned} $$ Then the algorithm could be (there is no contribution, except the GANs optimization part.) \\(\\overline{\\text { Algorithm } 1 \\text { Federated Least Squares Generative Adversarial }}\\) Networks (Fed-LSGAN) Require: Learning rate \\(\\alpha\\) ; Adam hyperparameters \\(\\beta_1, \\beta_2\\) ; Local minibatch size \\(m\\) ; Global epoch \\(W\\) ; Synchronization interval \\(K\\) , Client ratio \\(E\\) . Require: A global LSGANs model with parameters \\(\\left(\\theta_g^{\\mathcal{S}}, \\theta_d^{\\mathcal{S}}\\right)\\) for Discriminator and Generator on central server \\(\\mathcal{S}\\) ; local LSGANs models with parameters \\(\\left\\{\\theta_g, \\theta_d\\right\\}_{i=1}^{N_r}\\) on \\(N_r\\) clients \\(\\{\\mathcal{C}\\}_{i=1}^{N_r}\\) . 8: Update generator \\(\\theta_g^e\\) of client \\(e\\) - Sample batch noise samples \\(\\{z\\}_{i=1}^m \\sim P_Z\\) . - Update parameters for generator network $$ \\left{\\begin{array}{l} g_{\\theta_g} \\leftarrow \\nabla_{\\theta_g} \\frac{1}{m} \\sum_{i=1}^m \\frac{1}{2}\\left(D\\left(G\\left(z^{(i)}\\right)\\right)-1\\right)^2 \\ \\theta_g \\leftarrow \\theta_g-\\alpha \\cdot \\operatorname{Adam}\\left(\\theta_g, g_{\\theta_g}, \\beta_1, \\beta_2\\right) \\end{array}\\right. $$ 11: end for 12: \\(\\quad\\) if \\(w \\bmod K=0\\) then 13: All selected clients send parameters to server, and the server aggregate \\(\\theta^{\\mathcal{S}}\\) by averaging $$ \\theta_g^S \\leftarrow \\frac{1}{N_e} \\sum_{e=1}^{N_e} \\theta_g^e, \\quad \\theta_d^{\\mathcal{S}} \\leftarrow \\frac{1}{N_e} \\sum_{e=1}^{N_e} \\theta_d^e $$ 14: The server send back parameters and clients update local parameters $$ \\left{\\theta_g, \\theta_d\\right}_{i=1}^{N_r} \\leftarrow\\left(\\theta_g^{\\mathcal{S}}, \\theta_d^{\\mathcal{S}}\\right) $$ 15: end if Metric: Performance metric Correlation Analysis: $$ R(\\tau)=\\frac{\\mathbb{E}\\left[\\left(S_t-\\mu\\right)\\left(S_{t+\\tau}-\\mu\\right)\\right]}{\\sigma^2} $$ where \\(S\\) is a random time series; \\(\\mu\\) and \\(\\sigma\\) denote the mean and variance of \\(S\\) , respectively; and \\(\\tau\\) is the time lag. We use the continuous ranked probability score ( CRPS ) which measures the dissimilarity of the cumulative distributions between generated scenarios and historical observations. The score at lead time \\(\\ell\\) is defined as $$ \\operatorname{CRPS} l=\\frac{1}{M} \\sum {t=1}^M \\int_0^1\\left(\\widehat{F} {t+l \\mid t}(\\xi)-\\mathbf{1}\\left(\\xi \\geq \\xi {t+l}\\right)\\right)^2 d \\xi $$ where \\(M\\) is the total number of scenarios, \\(\\widehat{F}_{t+l \\mid t}(\\xi)\\) denotes the cumulative distribution function of normalized scenario, and \\(\\mathbf{1}\\left(\\xi \\geq \\xi_{t+l}\\right)\\) is the indicator function for comparing scenarios and observation. Fr\u00e9chet inception Distance (FID) $$ \\operatorname{FID}\\left(P_d, P_g\\right)=\\left|\\mu_d-\\mu_g\\right|+\\operatorname{Tr}\\left(\\Sigma_d+\\Sigma_g-2\\left(\\Sigma_d \\Sigma_g\\right)^{\\frac{1}{2}}\\right) $$ where \\(\\mu_d\\) and \\(\\mu_g\\) represent the empirical mean; \\(\\Sigma_d\\) and \\(\\Sigma_g\\) are empirical covariance. Kernel Maximum Mean Discrepancy (MMD): measures the difference between \\(P_d\\) and \\(P_g\\) for some fixed kernel function \\(k\\) , which is defined as $$ \\operatorname{MMD}^2\\left(P_d, P_g\\right)=\\underset{\\substack{x, x^{\\prime} \\sim P_d \\ y, y^{\\prime} \\sim P_g}}{ }\\left[k\\left(x, x^{\\prime}\\right)-2 k(x, y)+k\\left(y, y^{\\prime}\\right)\\right] $$ The 1-Nearest Neighbor classifier Energy Score (ES) $$ \\mathrm{ES}=\\frac{1}{M} \\sum_{i=1}^M\\left|\\varsigma-\\xi_i\\right|-\\frac{1}{2 M^2} \\sum_{i=1}^M \\sum_{j=1}^M\\left|\\xi_i-\\xi_j\\right| $$ \\(\\varsigma\\) is the real renewable power output, \\(\\xi_i\\) is the \\(i\\) -th generated time series scenario and \\(M\\) denotes the number of scenarios. Pearson correlation coefficient \\(\\rho\\) of two time series \\(S_i\\) and \\(S_j\\) is $$ \\rho\\left(S_i, S_j\\right)=\\frac{\\sum_{i=1}^n\\left(S_i-\\bar{S} i\\right)\\left(S_j-\\bar{S}_j\\right)}{\\sqrt{\\sum {i=1}^n\\left(S_i-\\bar{S} i\\right)^2} \\sqrt{\\sum {i=1}^n\\left(S_j-\\bar{S}_j\\right)^2}} $$","title":"Paper 2 Title:"},{"location":"Research/Paper%20Title%20Deterministic%20and%20Probabilistic%20Wind%20Power%20Forecasts%20by%20Considering%20Various%20Atmospheric%20Models%20and%20Feature/#_1","text":"","title":""},{"location":"Research/dataset%20description/","text":"Wind Power Generation Dataset # \u98ce\u529b\u529f\u7387\u6570\u636e\u96c6\u7684\u4e00\u4e9b\u7279\u70b9 # Sample Time # These data is collected from the Supervisory Control and Data Acquisition (SCADA) system. The SCADA data are sampled every 10 minutes from each wind turbine in the wind farm which consists of 134 wind turbines. The statistic of this dataset is shown below. Days Interval # of columns # of turbines # of records 245 10minutes 13 134 4,727,520 This dataset includes critical external features, such as wind speed, wind direction, and external temperature, that influence the wind power generation; as well as essential internal features, such as the inside temperature, nacelle direction and Pitch angle of blades, which can indicate the operating status of each wind turbine. Evaluation # We aim at addressing the forecasting ahead of 48 hours. Fore example, given at 6:00 A.M. today, it is required to effectively forecast the wind power generation beginning from 6:00 A.M. on this day to 5:50 AM on the day after tomorrow, given a series of historical records of the wind farm and the related wind turbines. It is required to output the predicted values every 10 minutes. To be specific, at one time point, it is required to predict a future length-280 wind power supply time-series. The average of RMSE (Root Mean Square Error) and MAE (Mean Absolute Error) is used as the main evaluation score. Caveats about the data # Zero values (if active power and reactive power <0, make them zero) Missing values Unknown values (In some time, the wind turbines are stopped to generate power by external reasons such as wind turbine renovation and/or actively scheduling the powering to avoid overloading the grid; if \\(P a t v \\leq 0\\) and \\(W s p d>2.5\\) , then power is unknown ); if \\(P a b 1>89^{\\circ}\\) or \\(P a b 2>89^{\\circ}\\) or \\(P a b 3>89^{\\circ}\\) , then the actual active power of this wind is unknown. Abnormal values (Nidir \\(>720^{\\circ}\\) or Nidir \\(<-720^{\\circ}\\) , then the actual active power is unknown; Widr \\(>180^{\\circ}\\) or Widr \\(<-180^{\\circ}\\) , then the actual active power is unknown.)","title":"Dataset description"},{"location":"Research/dataset%20description/#wind-power-generation-dataset","text":"","title":"Wind Power Generation Dataset"},{"location":"Research/dataset%20description/#_1","text":"","title":"\u98ce\u529b\u529f\u7387\u6570\u636e\u96c6\u7684\u4e00\u4e9b\u7279\u70b9"},{"location":"Research/dataset%20description/#sample-time","text":"These data is collected from the Supervisory Control and Data Acquisition (SCADA) system. The SCADA data are sampled every 10 minutes from each wind turbine in the wind farm which consists of 134 wind turbines. The statistic of this dataset is shown below. Days Interval # of columns # of turbines # of records 245 10minutes 13 134 4,727,520 This dataset includes critical external features, such as wind speed, wind direction, and external temperature, that influence the wind power generation; as well as essential internal features, such as the inside temperature, nacelle direction and Pitch angle of blades, which can indicate the operating status of each wind turbine.","title":"Sample Time"},{"location":"Research/dataset%20description/#evaluation","text":"We aim at addressing the forecasting ahead of 48 hours. Fore example, given at 6:00 A.M. today, it is required to effectively forecast the wind power generation beginning from 6:00 A.M. on this day to 5:50 AM on the day after tomorrow, given a series of historical records of the wind farm and the related wind turbines. It is required to output the predicted values every 10 minutes. To be specific, at one time point, it is required to predict a future length-280 wind power supply time-series. The average of RMSE (Root Mean Square Error) and MAE (Mean Absolute Error) is used as the main evaluation score.","title":"Evaluation"},{"location":"Research/dataset%20description/#caveats-about-the-data","text":"Zero values (if active power and reactive power <0, make them zero) Missing values Unknown values (In some time, the wind turbines are stopped to generate power by external reasons such as wind turbine renovation and/or actively scheduling the powering to avoid overloading the grid; if \\(P a t v \\leq 0\\) and \\(W s p d>2.5\\) , then power is unknown ); if \\(P a b 1>89^{\\circ}\\) or \\(P a b 2>89^{\\circ}\\) or \\(P a b 3>89^{\\circ}\\) , then the actual active power of this wind is unknown. Abnormal values (Nidir \\(>720^{\\circ}\\) or Nidir \\(<-720^{\\circ}\\) , then the actual active power is unknown; Widr \\(>180^{\\circ}\\) or Widr \\(<-180^{\\circ}\\) , then the actual active power is unknown.)","title":"Caveats about the data"},{"location":"Research/dataset_description/","text":"Wind Power Generation Dataset # \u98ce\u529b\u529f\u7387\u6570\u636e\u96c6\u7684\u4e00\u4e9b\u7279\u70b9 # Sample Time # These data is collected from the Supervisory Control and Data Acquisition (SCADA) system. The SCADA data are sampled every 10 minutes from each wind turbine in the wind farm which consists of 134 wind turbines. The statistic of this dataset is shown below. Days Interval # of columns # of turbines # of records 245 10minutes 13 134 4,727,520 This dataset includes critical external features, such as wind speed, wind direction, and external temperature, that influence the wind power generation; as well as essential internal features, such as the inside temperature, nacelle direction and Pitch angle of blades, which can indicate the operating status of each wind turbine. Evaluation # We aim at addressing the forecasting ahead of 48 hours. Fore example, given at 6:00 A.M. today, it is required to effectively forecast the wind power generation beginning from 6:00 A.M. on this day to 5:50 AM on the day after tomorrow, given a series of historical records of the wind farm and the related wind turbines. It is required to output the predicted values every 10 minutes. To be specific, at one time point, it is required to predict a future length-280 wind power supply time-series. The average of RMSE (Root Mean Square Error) and MAE (Mean Absolute Error) is used as the main evaluation score. Caveats about the data # Zero values (if active power and reactive power <0, make them zero) Missing values Unknown values (In some time, the wind turbines are stopped to generate power by external reasons such as wind turbine renovation and/or actively scheduling the powering to avoid overloading the grid; if \\(P a t v \\leq 0\\) and \\(W s p d>2.5\\) , then power is unknown ); if \\(P a b 1>89^{\\circ}\\) or \\(P a b 2>89^{\\circ}\\) or \\(P a b 3>89^{\\circ}\\) , then the actual active power of this wind is unknown. Abnormal values (Nidir \\(>720^{\\circ}\\) or Nidir \\(<-720^{\\circ}\\) , then the actual active power is unknown; Widr \\(>180^{\\circ}\\) or Widr \\(<-180^{\\circ}\\) , then the actual active power is unknown.)","title":"Wind energy dataset"},{"location":"Research/dataset_description/#wind-power-generation-dataset","text":"","title":"Wind Power Generation Dataset"},{"location":"Research/dataset_description/#_1","text":"","title":"\u98ce\u529b\u529f\u7387\u6570\u636e\u96c6\u7684\u4e00\u4e9b\u7279\u70b9"},{"location":"Research/dataset_description/#sample-time","text":"These data is collected from the Supervisory Control and Data Acquisition (SCADA) system. The SCADA data are sampled every 10 minutes from each wind turbine in the wind farm which consists of 134 wind turbines. The statistic of this dataset is shown below. Days Interval # of columns # of turbines # of records 245 10minutes 13 134 4,727,520 This dataset includes critical external features, such as wind speed, wind direction, and external temperature, that influence the wind power generation; as well as essential internal features, such as the inside temperature, nacelle direction and Pitch angle of blades, which can indicate the operating status of each wind turbine.","title":"Sample Time"},{"location":"Research/dataset_description/#evaluation","text":"We aim at addressing the forecasting ahead of 48 hours. Fore example, given at 6:00 A.M. today, it is required to effectively forecast the wind power generation beginning from 6:00 A.M. on this day to 5:50 AM on the day after tomorrow, given a series of historical records of the wind farm and the related wind turbines. It is required to output the predicted values every 10 minutes. To be specific, at one time point, it is required to predict a future length-280 wind power supply time-series. The average of RMSE (Root Mean Square Error) and MAE (Mean Absolute Error) is used as the main evaluation score.","title":"Evaluation"},{"location":"Research/dataset_description/#caveats-about-the-data","text":"Zero values (if active power and reactive power <0, make them zero) Missing values Unknown values (In some time, the wind turbines are stopped to generate power by external reasons such as wind turbine renovation and/or actively scheduling the powering to avoid overloading the grid; if \\(P a t v \\leq 0\\) and \\(W s p d>2.5\\) , then power is unknown ); if \\(P a b 1>89^{\\circ}\\) or \\(P a b 2>89^{\\circ}\\) or \\(P a b 3>89^{\\circ}\\) , then the actual active power of this wind is unknown. Abnormal values (Nidir \\(>720^{\\circ}\\) or Nidir \\(<-720^{\\circ}\\) , then the actual active power is unknown; Widr \\(>180^{\\circ}\\) or Widr \\(<-180^{\\circ}\\) , then the actual active power is unknown.)","title":"Caveats about the data"},{"location":"Research/feature_engineering/","text":"\u65f6\u95f4\u5e8f\u5217\u91cc\u9762\u7684\u7279\u5f81\u5de5\u7a0b #","title":"Feature engineering"},{"location":"Research/feature_engineering/#_1","text":"","title":"\u65f6\u95f4\u5e8f\u5217\u91cc\u9762\u7684\u7279\u5f81\u5de5\u7a0b"},{"location":"Research/we/","text":"fasdfasd","title":"If you can"}]}